{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to MkDocs For full documentation visit mkdocs.org . Commands mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit. Project layout mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files. About This documentation contains details of Python Module, Devops Applications and Cheat commands of tools.","title":"Home"},{"location":"#welcome-to-mkdocs","text":"For full documentation visit mkdocs.org .","title":"Welcome to MkDocs"},{"location":"#commands","text":"mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit.","title":"Commands"},{"location":"#project-layout","text":"mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Project layout"},{"location":"#about","text":"This documentation contains details of Python Module, Devops Applications and Cheat commands of tools.","title":"About"},{"location":"ansible/","text":"Ansible How Ansible works Introduction Usage Architecture","title":"Ansible"},{"location":"ansible/#ansible","text":"","title":"Ansible"},{"location":"ansible/#how-ansible-works","text":"","title":"How Ansible works"},{"location":"ansible/#introduction","text":"","title":"Introduction"},{"location":"ansible/#usage","text":"","title":"Usage"},{"location":"ansible/#architecture","text":"","title":"Architecture"},{"location":"bullets/","text":"Commands Commands Python commands Docker commands Git commands Kubernetes commands Helm commands Openshift commands Python commands Execute Command Create virtual env python -m venv venv Activate virtual environment Windows ./venv/bin/activate Linux source venv/bin/activate Deactivate virtual environment deactivate Install requirements.txt pip install -r requirements.txt Git clone with pip pip install Docker commands Execute Command Git configuration/set username git config--global user.name \"<User name>\" Git commands Execute Command Git configuration/set username git config--global user.name \"<User name>\" Git configuration/set email git config --global user.email \"xyz123@gmail.com\" Git configuration list git config-list Git init git init <Repo Name> Git clone git clone <remote Url> git clone particular branch git clone -b <branch-name> <git-url> Create branch git branch <branch name> List all branch names git branch -a git branch --list Checkout branch git checkout <branch name> Checkout and switch to new branch git checkout -b <branchname> Git add single file git add <Filename> <Filename> Git add all files git add . Git status git status Git commit git commit -m \" Commit Message\" Git add+commit git commit -am \" Commit Message\" Git push git push Delete branch git branch -d <branch_name> Delete remote branch git push origin-delete <branch name> Git commit history git log Rename branch git branch -m <old branch name> <new branch name> Merge branch vscode > git clone url git checkout main branch git pull git checkout feature branch git merge main_branch ## if you want to update feature branch with main ## you will get merge conflicts in vscode ## resolve merge conflicts git commit ##check branch name and commit to your feature branch git push feature branch Display the modified files git log --stat Display the modification on each line of a file git blame <filename> Track changes that have not been staged git diff Track changes that have staged but not committed git diff --staged Track the changes after committing a file git diff HEAD Track the changes between two commits git diff <commit1-sha> <commit2-sha> Git Diff Branches git diff <branch 1> < branch 2> Kubernetes commands Execute Command To start Minikube minikube start Create a namespace kubectl create namespace development Create a deployment using kubectl command kubectl create deployment \"nameofdeployment\" --image=\"imagename\" Example: kubectl create deployment nginx-deploy --image=nginx Deploy using a YAML file kubectl apply -f deployment.yaml Display nodes kubectl get nodes Display services kubectl get services Display pods kubectl get pods Display describe pods kubectl describe pod \"pod-name\" Display pods with a specific namespace kubectl get pods -n \"namespace\" Display deployments kubectl get deployment Display ReplicaSets kubectl get replicaset Display config maps kubectl get cm Display Get Jobs kubectl get jobs -n namespace Display Scaled Jobs kubectl describe scaledjob <scaledjob-name> -n <namespace> Display Secret Provider class kubectl get secretproviderclass Display service status kubectl describe service \"service-name\" Display changes of a config map kubectl describe cm \"release-name\"-configmap Switch to a different namespace kubectl config set-context --current --namespace=\"namespace\" Display deployment file snippet in VS Code $env:KUBE_EDITOR=\"code --wait\" > kubectl edit deployment \"deployment-name\" Display pod status kubectl get pods kubectl describe pod \"pod-name\" Display Secrets kubectl get secret Display describe secrets kubectl describe secret \"secret-name\" Debug pod status with a specific namespace kubectl describe pod \"pod-name\" -n development Debug pods with external IP addresses kubectl get pod -o wide Display pod logs kubectl logs \"pod-name\" -n development Interact with pods kubectl exec -it \"pod-name\" -- /bin/bash Delete all pods kubectl delete pods --all -n development Delete deployments kubectl delete deployment \"deployment-name\" Helm commands Execute Commands Helm repo add bitnami helm repo add bitnami https://charts.bitnami.com/bitnami Helm repo update helm repo update Helm repo list helm repo list Minikube start minikube start Kubectl create namespace kubectl create ns \"namespace\" Helm install kube-state-metrics helm install kube-state-metrics bitnami/kube-state-metrics -n metrics Helm create chart helm create \"chart-name\" Helm lint helm lint . Helm template with debug helm template --dry-run --debug \"release-name\" . Helm install status helm ls -n \"namespace\" Kubectl get all kubectl get all -n \"namespace\" Helm install with namespace helm install demo-001 . -n development Helm upgrade helm upgrade \"release-name\" . Helm upgrade with namespace helm upgrade demo-001 . -n development Helm history helm history \"release-name\" Helm rollback helm rollback \"release-name\" Helm rollback with revision helm rollback \"release-name\" \"revision-number\" Helm upgrade with specific version helm upgrade kube-state-metrics bitnami/kube-state-metrics --version 0.4.0 -n metrics Helm delete helm delete \"release-name\" Helm install with updated values helm install \"release-name\" --set data.type=\"9090\" Kubectl port-forward for kube-state-metrics kubectl port-forward svc/kube-state-metrics 8080:8080 -n metrics Helm show chart helm show chart bitnami/kube-state-metrics Helm show values helm show values bitnami/kube-state-metrics Helm uninstall helm uninstall \"release-name\" . -n \"namespace\" Openshift commands Execute Command OpenShift login oc login Create new project oc new-project <project-name> Switch to specific project oc project <project-name> Check the project name oc project Current status of project oc status Display pods oc get pods Describe pod oc describe pod <pod-name> Display logs oc logs <pod-name> Display service of project goc get svc Display specific service of project oc describe svc <service-name> Expose a service to the internet oc expose svc <service-name> Delete a specific service. oc delete svc <service-name> Create new application oc new-app Edit a deployment configuration oc edit dc <deployment-config> Scale a deployment configuration oc scale dc <deployment-config> --replicas=<number> Rollout to latest version of deployment configuration oc rollout latest <deployment-config>","title":"Bullets"},{"location":"bullets/#commands","text":"Commands Python commands Docker commands Git commands Kubernetes commands Helm commands Openshift commands","title":"Commands"},{"location":"bullets/#python-commands","text":"Execute Command Create virtual env python -m venv venv Activate virtual environment Windows ./venv/bin/activate Linux source venv/bin/activate Deactivate virtual environment deactivate Install requirements.txt pip install -r requirements.txt Git clone with pip pip install","title":"Python commands"},{"location":"bullets/#docker-commands","text":"Execute Command Git configuration/set username git config--global user.name \"<User name>\"","title":"Docker commands"},{"location":"bullets/#git-commands","text":"Execute Command Git configuration/set username git config--global user.name \"<User name>\" Git configuration/set email git config --global user.email \"xyz123@gmail.com\" Git configuration list git config-list Git init git init <Repo Name> Git clone git clone <remote Url> git clone particular branch git clone -b <branch-name> <git-url> Create branch git branch <branch name> List all branch names git branch -a git branch --list Checkout branch git checkout <branch name> Checkout and switch to new branch git checkout -b <branchname> Git add single file git add <Filename> <Filename> Git add all files git add . Git status git status Git commit git commit -m \" Commit Message\" Git add+commit git commit -am \" Commit Message\" Git push git push Delete branch git branch -d <branch_name> Delete remote branch git push origin-delete <branch name> Git commit history git log Rename branch git branch -m <old branch name> <new branch name> Merge branch vscode > git clone url git checkout main branch git pull git checkout feature branch git merge main_branch ## if you want to update feature branch with main ## you will get merge conflicts in vscode ## resolve merge conflicts git commit ##check branch name and commit to your feature branch git push feature branch Display the modified files git log --stat Display the modification on each line of a file git blame <filename> Track changes that have not been staged git diff Track changes that have staged but not committed git diff --staged Track the changes after committing a file git diff HEAD Track the changes between two commits git diff <commit1-sha> <commit2-sha> Git Diff Branches git diff <branch 1> < branch 2>","title":"Git commands"},{"location":"bullets/#kubernetes-commands","text":"Execute Command To start Minikube minikube start Create a namespace kubectl create namespace development Create a deployment using kubectl command kubectl create deployment \"nameofdeployment\" --image=\"imagename\" Example: kubectl create deployment nginx-deploy --image=nginx Deploy using a YAML file kubectl apply -f deployment.yaml Display nodes kubectl get nodes Display services kubectl get services Display pods kubectl get pods Display describe pods kubectl describe pod \"pod-name\" Display pods with a specific namespace kubectl get pods -n \"namespace\" Display deployments kubectl get deployment Display ReplicaSets kubectl get replicaset Display config maps kubectl get cm Display Get Jobs kubectl get jobs -n namespace Display Scaled Jobs kubectl describe scaledjob <scaledjob-name> -n <namespace> Display Secret Provider class kubectl get secretproviderclass Display service status kubectl describe service \"service-name\" Display changes of a config map kubectl describe cm \"release-name\"-configmap Switch to a different namespace kubectl config set-context --current --namespace=\"namespace\" Display deployment file snippet in VS Code $env:KUBE_EDITOR=\"code --wait\" > kubectl edit deployment \"deployment-name\" Display pod status kubectl get pods kubectl describe pod \"pod-name\" Display Secrets kubectl get secret Display describe secrets kubectl describe secret \"secret-name\" Debug pod status with a specific namespace kubectl describe pod \"pod-name\" -n development Debug pods with external IP addresses kubectl get pod -o wide Display pod logs kubectl logs \"pod-name\" -n development Interact with pods kubectl exec -it \"pod-name\" -- /bin/bash Delete all pods kubectl delete pods --all -n development Delete deployments kubectl delete deployment \"deployment-name\"","title":"Kubernetes commands"},{"location":"bullets/#helm-commands","text":"Execute Commands Helm repo add bitnami helm repo add bitnami https://charts.bitnami.com/bitnami Helm repo update helm repo update Helm repo list helm repo list Minikube start minikube start Kubectl create namespace kubectl create ns \"namespace\" Helm install kube-state-metrics helm install kube-state-metrics bitnami/kube-state-metrics -n metrics Helm create chart helm create \"chart-name\" Helm lint helm lint . Helm template with debug helm template --dry-run --debug \"release-name\" . Helm install status helm ls -n \"namespace\" Kubectl get all kubectl get all -n \"namespace\" Helm install with namespace helm install demo-001 . -n development Helm upgrade helm upgrade \"release-name\" . Helm upgrade with namespace helm upgrade demo-001 . -n development Helm history helm history \"release-name\" Helm rollback helm rollback \"release-name\" Helm rollback with revision helm rollback \"release-name\" \"revision-number\" Helm upgrade with specific version helm upgrade kube-state-metrics bitnami/kube-state-metrics --version 0.4.0 -n metrics Helm delete helm delete \"release-name\" Helm install with updated values helm install \"release-name\" --set data.type=\"9090\" Kubectl port-forward for kube-state-metrics kubectl port-forward svc/kube-state-metrics 8080:8080 -n metrics Helm show chart helm show chart bitnami/kube-state-metrics Helm show values helm show values bitnami/kube-state-metrics Helm uninstall helm uninstall \"release-name\" . -n \"namespace\"","title":"Helm commands"},{"location":"bullets/#openshift-commands","text":"Execute Command OpenShift login oc login Create new project oc new-project <project-name> Switch to specific project oc project <project-name> Check the project name oc project Current status of project oc status Display pods oc get pods Describe pod oc describe pod <pod-name> Display logs oc logs <pod-name> Display service of project goc get svc Display specific service of project oc describe svc <service-name> Expose a service to the internet oc expose svc <service-name> Delete a specific service. oc delete svc <service-name> Create new application oc new-app Edit a deployment configuration oc edit dc <deployment-config> Scale a deployment configuration oc scale dc <deployment-config> --replicas=<number> Rollout to latest version of deployment configuration oc rollout latest <deployment-config>","title":"Openshift commands"},{"location":"for_eg/","text":"- Args and Kwargs - Lambda - Filter - Mapping - List Comprehension - Decorator - Read Write File - Logging - Class - Isinstance - Dataclass - Method-Static - Inheritance Builtin Args and Kwargs ##args def sum(*args): total = 0 for i in args: total = total+i print(total) sum(5,6,7,8,9) ##we can pass n number of arguments ###for key word arguments def student(**data): for key,value in data.items(): print(f\"{key} {value}\") ##callable = student(key=value,key=value) student(name=\"John\",age=30) student(name=\"Ram\",age=26) Lambda ##passing argument in lambda nums=[2,4,6,8,10] def mul(x): for i in x: i=i*2 print(i,end=\" \") mul(nums) ##passing argument in lambda using map doubled_map = list(map(lambda x: x*2,nums)) print(doubled_map) Filter ##filter is similar to mapping nums_list = [1,2,3,4,5,6,7,8,9] even=[] for x in nums_list: if x % 2 == 0: even.append(x) print(even) #with filter def even(x): return x%2==0 even_list= list(filter(even,nums_list)) print(even_list) Mapping double_num = [] numbers = (5,6,7,8,9) for num in numbers: double_num.append(num * 2) print(double_num) ##using map def double(num): return num*2 double_List = list(map(double,numbers)) print(double_List) List Comprehension syntax > [return-value for-loop if-condition] nums_List=[5,6,7,8] ##listcomprehension for maping mapping = list(x*2 for x in nums_List) print(mapping) ##listcomprehension for filtering filtering = list(x for x in nums_List if x%2==0) print(filtering) Decorator def first(func): ##passing the func as an argument def second(): print(\"execute the first line\") func() ##call the funct print(\"execute the second line\") return second() @first ###decorate def middle(): print(\"execute the middle line\") # mid = first(middle) ##another way to call decorate middle Read Write File f = open(\"./read_write_demo.txt\",\"r\") ##r=read print(f.read()) ##read() to print the txt f.close() ##close Logging for creating a log file for each different py file for those whose function are imported from different py file import logging ##using getlogger to create seperate log files logger=logging.getLogger(__name__) logger.setLevel(logging.INFO) ##set logging level ## formatter is to set logs format log_format=logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\") ## fileHandler is user to create a file log_file=logging.FileHandler(\"example.log\") log_file.setFormatter(log_format) logger.addHandler(log_file) ## further code def something(): logger.info(\"this is a something function\") Class #basic class structure class Student: ##self hold the value of instant obj def __init__(self,name,age) -> None: ##here self represent to b1 self.name = name self.age = age def __str__(self) -> str: ##__str__() function controls what should be returned when the class object is representd as string. return f\"{self.name}\" def hello(self): ## funct is called methods in class print(\"heellooo\") def get_name(self): return self.name def get_details(self): print(\"name\",self.name) print(\"age\",self.age) ##for every class we need to define obj ##here b1 var is obj for class Book b1 = Student(\"RAM\",26) ##b1 here is obj b1.hello() ##obj.method print(b1.get_name()) ##obj.method b1.get_details() b2=Student(\"Krishna\",11) ##use the same class by creating another obj b2.get_details() Isinstance x = isinstance(\"Hello\", (float, int, str, list, dict, tuple)) print(x) class Myclass: name = \"John\" obj = Myclass() x = isinstance(obj, Myclass) Dataclass #without dataclass class Person(): def __init__(self, first_name, last_name, age): self.first_name = first_name self.last_name = last_name self.age = age person=Person(\"Ram\",\"krishna\",26) print(person.first_name) ##with dataclass from dataclasses import dataclass ##import dataclass @dataclass class Book: title : str ###variables are define without using __init__ instance author : str price : float book=Book(\"Peaceofmind\",\"Unknown\",50.50) print(book.price) Method-Static ##class method :A class method is a method which is bound to the class and not the object of the class. ##static method : A static method is used when we want to create a function without using self as instance-(just to create a independent fucntin) from datetime import date class Person: def __init__(self, name, age): self.name = name self.age = age # a class method to create a # Person object by birth year. @classmethod def fromBirthYear(cls, name, year): return cls(name, date.today().year - year) # a static method to check if a # Person is adult or not. @staticmethod def isAdult(age): return age > 18 ##static method eg 2 ##this function is independent of the class ,created without using self as instance @staticmethod def thankyou(msg): return msg person1 = Person('ram', 21) person2 = Person.fromBirthYear('ram', 1997) print(person1.age) print(person2.age) # print the result print(Person.isAdult(22)) # print thankyuu msg print(Person.thankyou(\"thanks for looking up this file\")) Inheritance class Publisher: def __init__(self,title,price) -> None: self.title=title self.price=price class Author(Publisher): ##add class name to inherit def __init__(self,title,price,pages,period) -> None: super().__init__(title,price) ###add module super()to fetch the var for Publisher class self.period=period self.pages=pages class Book1(Publisher): ##add class name to inherit def __init__(self,title,price,author) -> None: super().__init__(title,price) self.author=author ##adding variable rather then class class Magazine1(Author): def __init__(self,title,author,pages,period) -> None: ##using Author class to fetch the values super().__init__(title,author,pages,period) class Newspaper1(Author): def __init__(self,title,price,pages,period) -> None: super().__init__(title,price,pages,period) b1=Book1(\"PeaceofMind\",\"Unknown\",100) m1=Magazine1(\"Vogue\",\"Kiran\",20,15) n1=Newspaper1(\"TOI\",\"toi\",5,10) print(b1.author) print(m1.period)","title":"For eg"},{"location":"for_eg/#builtin","text":"","title":"Builtin"},{"location":"for_eg/#args-and-kwargs","text":"##args def sum(*args): total = 0 for i in args: total = total+i print(total) sum(5,6,7,8,9) ##we can pass n number of arguments ###for key word arguments def student(**data): for key,value in data.items(): print(f\"{key} {value}\") ##callable = student(key=value,key=value) student(name=\"John\",age=30) student(name=\"Ram\",age=26)","title":"Args and Kwargs"},{"location":"for_eg/#lambda","text":"##passing argument in lambda nums=[2,4,6,8,10] def mul(x): for i in x: i=i*2 print(i,end=\" \") mul(nums) ##passing argument in lambda using map doubled_map = list(map(lambda x: x*2,nums)) print(doubled_map)","title":"Lambda"},{"location":"for_eg/#filter","text":"##filter is similar to mapping nums_list = [1,2,3,4,5,6,7,8,9] even=[] for x in nums_list: if x % 2 == 0: even.append(x) print(even) #with filter def even(x): return x%2==0 even_list= list(filter(even,nums_list)) print(even_list)","title":"Filter"},{"location":"for_eg/#mapping","text":"double_num = [] numbers = (5,6,7,8,9) for num in numbers: double_num.append(num * 2) print(double_num) ##using map def double(num): return num*2 double_List = list(map(double,numbers)) print(double_List)","title":"Mapping"},{"location":"for_eg/#list-comprehension","text":"syntax > [return-value for-loop if-condition] nums_List=[5,6,7,8] ##listcomprehension for maping mapping = list(x*2 for x in nums_List) print(mapping) ##listcomprehension for filtering filtering = list(x for x in nums_List if x%2==0) print(filtering)","title":"List Comprehension"},{"location":"for_eg/#decorator","text":"def first(func): ##passing the func as an argument def second(): print(\"execute the first line\") func() ##call the funct print(\"execute the second line\") return second() @first ###decorate def middle(): print(\"execute the middle line\") # mid = first(middle) ##another way to call decorate middle","title":"Decorator"},{"location":"for_eg/#read-write-file","text":"f = open(\"./read_write_demo.txt\",\"r\") ##r=read print(f.read()) ##read() to print the txt f.close() ##close","title":"Read Write File"},{"location":"for_eg/#logging","text":"for creating a log file for each different py file for those whose function are imported from different py file import logging ##using getlogger to create seperate log files logger=logging.getLogger(__name__) logger.setLevel(logging.INFO) ##set logging level ## formatter is to set logs format log_format=logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\") ## fileHandler is user to create a file log_file=logging.FileHandler(\"example.log\") log_file.setFormatter(log_format) logger.addHandler(log_file) ## further code def something(): logger.info(\"this is a something function\")","title":"Logging"},{"location":"for_eg/#class","text":"#basic class structure class Student: ##self hold the value of instant obj def __init__(self,name,age) -> None: ##here self represent to b1 self.name = name self.age = age def __str__(self) -> str: ##__str__() function controls what should be returned when the class object is representd as string. return f\"{self.name}\" def hello(self): ## funct is called methods in class print(\"heellooo\") def get_name(self): return self.name def get_details(self): print(\"name\",self.name) print(\"age\",self.age) ##for every class we need to define obj ##here b1 var is obj for class Book b1 = Student(\"RAM\",26) ##b1 here is obj b1.hello() ##obj.method print(b1.get_name()) ##obj.method b1.get_details() b2=Student(\"Krishna\",11) ##use the same class by creating another obj b2.get_details()","title":"Class"},{"location":"for_eg/#isinstance","text":"x = isinstance(\"Hello\", (float, int, str, list, dict, tuple)) print(x) class Myclass: name = \"John\" obj = Myclass() x = isinstance(obj, Myclass)","title":"Isinstance"},{"location":"for_eg/#dataclass","text":"#without dataclass class Person(): def __init__(self, first_name, last_name, age): self.first_name = first_name self.last_name = last_name self.age = age person=Person(\"Ram\",\"krishna\",26) print(person.first_name) ##with dataclass from dataclasses import dataclass ##import dataclass @dataclass class Book: title : str ###variables are define without using __init__ instance author : str price : float book=Book(\"Peaceofmind\",\"Unknown\",50.50) print(book.price)","title":"Dataclass"},{"location":"for_eg/#method-static","text":"##class method :A class method is a method which is bound to the class and not the object of the class. ##static method : A static method is used when we want to create a function without using self as instance-(just to create a independent fucntin) from datetime import date class Person: def __init__(self, name, age): self.name = name self.age = age # a class method to create a # Person object by birth year. @classmethod def fromBirthYear(cls, name, year): return cls(name, date.today().year - year) # a static method to check if a # Person is adult or not. @staticmethod def isAdult(age): return age > 18 ##static method eg 2 ##this function is independent of the class ,created without using self as instance @staticmethod def thankyou(msg): return msg person1 = Person('ram', 21) person2 = Person.fromBirthYear('ram', 1997) print(person1.age) print(person2.age) # print the result print(Person.isAdult(22)) # print thankyuu msg print(Person.thankyou(\"thanks for looking up this file\"))","title":"Method-Static"},{"location":"for_eg/#inheritance","text":"class Publisher: def __init__(self,title,price) -> None: self.title=title self.price=price class Author(Publisher): ##add class name to inherit def __init__(self,title,price,pages,period) -> None: super().__init__(title,price) ###add module super()to fetch the var for Publisher class self.period=period self.pages=pages class Book1(Publisher): ##add class name to inherit def __init__(self,title,price,author) -> None: super().__init__(title,price) self.author=author ##adding variable rather then class class Magazine1(Author): def __init__(self,title,author,pages,period) -> None: ##using Author class to fetch the values super().__init__(title,author,pages,period) class Newspaper1(Author): def __init__(self,title,price,pages,period) -> None: super().__init__(title,price,pages,period) b1=Book1(\"PeaceofMind\",\"Unknown\",100) m1=Magazine1(\"Vogue\",\"Kiran\",20,15) n1=Newspaper1(\"TOI\",\"toi\",5,10) print(b1.author) print(m1.period)","title":"Inheritance"},{"location":"git_cheat_cmd/","text":"Cheat Code For Tools Git Cheat Sheet 1. Git configuration Get and set configuration variables that control all facets of how Git looks and operates. Set the name: $ git config--global user.name \"<User name>\" Set the email: $ git config --global user.email \"xyz123@gmail.com\" Set the default editor: $ git config--global core.editor Vim Check the setting: $ git config-list 2. Git alias Set up an alias for each command: $ git config --global alias.co checkout $ git config --global alias.br branch $ git config --global alias.ci commit $ git config --global alias.st status 3. Starting a project Git init Create a local repository $ git init <Repo Name> Git clone Make a local copy of the server repository. $ git clone <remote Url> 4. Local changes Git add Add a file to staging (Index) area $ git add <Filename> <Filename> Add all files of a repo to staging (Index) area $ git add . Git commit Record or snapshots the file permanently in the version history with a message $ git commit -m \" Commit Message\" 5. Track changes Track the changes that have not been staged: $ git diff Track the changes that have staged but not committed: $ git diff --staged Track the changes after committing a file: $ git diff HEAD Track the changes between two commits: $ git diff <commit1-sha> <commit2-sha> Git Diff Branches: $ git diff <branch 1> < branch 2> 6.Git status Display the state of the working directory and the staging area. $ git status Shows objects: $ git show <options> <objects> 7. Commit History Git log Display the most recent commits and the status of the head: $ git log Display the output as one commit per line: $ git log -oneline Displays the files that have been modified: $ git log -stat Display the modified files with location: $ git log -p Git blame Display the modification on each line of a file: $ git blame <file name> 8. Ignoring files Create.gitignore: $ touch .gitignore Specify intentionally untracked files that Git should ignore. $ git Is-files -i --exclude-standard 9. Branching Create branch: $ git branch <branch name> List Branch: $ git branch --list Delete Branch: $ git branch -d <branch_name> Delete a remote Branch: $ git push origin-delete <branch name> Rename Branch: $ git branch -m <old branch name> <new branch name> 10.Git checkout Switch between branches a repository. Switch to a particular branch: $ git checkout <branch name> Create a new branch and switch to it: $ git checkout -b <branchname> Checkout a Remote branch: $ git checkout <remotebranch> 11. Git stash Switch branches without committing the current branch. Stash current work: $ git stash Saving stashes with a message: $ git stash save \"<Stashing Message\" Check the stored stashes: $ git stash list Re-apply the changes that you just stashed $ git stash apply Track the stashes and their changes: $ git stash show Re-apply the previous commits: $ git stash pop Delete a most recent stash from the queue: $ git stash drop Delete all the available stashes at once: $ git stash clear Stash work on a separate branch: $ git stash branch <branch name> Git cherry pic Apply the changes introduced by some existing commit: $ git cherry-pick <commit id> 12. Git merge Merge the branches: $ git merge <branch name> Merge the specified commit to currently active branch: $ git merge <commit> For instance: vscode > git clone url git checkout main branch git pull git checkout feature branch git merge main_branch ## if you want to update feature branch with main ## you will get merge conflicts in vscode ## resolve merge conflicts git commit ##check branch name and commit to your feature branch git push feature branch 13.Git rebase Apply a sequence of commits from distinct branches into a final commit. $ git rebase <branch name> Continue the rebasing process: $ git rebase-continue Abort the rebasing process: $ git rebase --skip Git interactive rebase Allow various operations like edit, rewrite, reorder, and more on existing commits. $ git rebase -i 14. Git remote Check the configuration of the remote server: $ git remote -v Add a remote for the repository: $ git remote add <short name> <remote URL> Fetch the data from remote server $ git fetch <Remote> Remove a remote connection from the repository: $ git remote rm <destination> Rename remote server: $ git remote rename <old name> <new name> Show additional information about a particular remote: $ git remote show <remote> Change remote: $ git remote set-url <remote name> <newURL> 15. Git origin master Push data to remote server: $ git push origin master Pull data from remote server: $ git pull origin master 16. Pushing Updates Transfer the commits from your local Push data to remote server: $ git push origin master Force push data: $ git push <remote> <branch> -f pository to a remote server. Delete a remote branch by push command: $ git push origin -delete edited 17. Pulling updates Pull the data from the server: $ git pull origin master Pull a remote branch: $ git pull <remote branch URL> 18. Git fetch Downloads branches and tags from one or more repositories. Fetch the remote repository: $ git fetch< repository Url> Fetch a specific branch: $ git fetch <branch URL><branch name> Fetch all the branches simultaneously: $ git fetch-all Synchronize the local repository: $ git fetch origin 19. Undo changes > Git revert Undo the changes $ git revert Revert a particular commit: $ git revert <commit-ish> 20. Git reset Reset the changes: $ git reset -hard $ git reset -soft $ git reset --mixed","title":"Git cheat"},{"location":"git_cheat_cmd/#cheat-code-for-tools","text":"","title":"Cheat Code For Tools"},{"location":"git_cheat_cmd/#git-cheat-sheet","text":"","title":"Git Cheat Sheet"},{"location":"git_cheat_cmd/#1-git-configuration","text":"Get and set configuration variables that control all facets of how Git looks and operates. Set the name: $ git config--global user.name \"<User name>\" Set the email: $ git config --global user.email \"xyz123@gmail.com\" Set the default editor: $ git config--global core.editor Vim Check the setting: $ git config-list","title":"1. Git configuration"},{"location":"git_cheat_cmd/#2-git-alias","text":"Set up an alias for each command: $ git config --global alias.co checkout $ git config --global alias.br branch $ git config --global alias.ci commit $ git config --global alias.st status","title":"2. Git alias"},{"location":"git_cheat_cmd/#3-starting-a-project","text":"Git init Create a local repository $ git init <Repo Name> Git clone Make a local copy of the server repository. $ git clone <remote Url>","title":"3. Starting a project"},{"location":"git_cheat_cmd/#4-local-changes","text":"Git add Add a file to staging (Index) area $ git add <Filename> <Filename> Add all files of a repo to staging (Index) area $ git add . Git commit Record or snapshots the file permanently in the version history with a message $ git commit -m \" Commit Message\"","title":"4. Local changes"},{"location":"git_cheat_cmd/#5-track-changes","text":"Track the changes that have not been staged: $ git diff Track the changes that have staged but not committed: $ git diff --staged Track the changes after committing a file: $ git diff HEAD Track the changes between two commits: $ git diff <commit1-sha> <commit2-sha> Git Diff Branches: $ git diff <branch 1> < branch 2>","title":"5. Track changes"},{"location":"git_cheat_cmd/#6git-status","text":"Display the state of the working directory and the staging area. $ git status Shows objects: $ git show <options> <objects>","title":"6.Git status"},{"location":"git_cheat_cmd/#7-commit-history","text":"Git log Display the most recent commits and the status of the head: $ git log Display the output as one commit per line: $ git log -oneline Displays the files that have been modified: $ git log -stat Display the modified files with location: $ git log -p Git blame Display the modification on each line of a file: $ git blame <file name>","title":"7. Commit History"},{"location":"git_cheat_cmd/#8-ignoring-files","text":"Create.gitignore: $ touch .gitignore Specify intentionally untracked files that Git should ignore. $ git Is-files -i --exclude-standard","title":"8. Ignoring files"},{"location":"git_cheat_cmd/#9-branching","text":"Create branch: $ git branch <branch name> List Branch: $ git branch --list Delete Branch: $ git branch -d <branch_name> Delete a remote Branch: $ git push origin-delete <branch name> Rename Branch: $ git branch -m <old branch name> <new branch name>","title":"9. Branching"},{"location":"git_cheat_cmd/#10git-checkout","text":"Switch between branches a repository. Switch to a particular branch: $ git checkout <branch name> Create a new branch and switch to it: $ git checkout -b <branchname> Checkout a Remote branch: $ git checkout <remotebranch>","title":"10.Git checkout"},{"location":"git_cheat_cmd/#11-git-stash","text":"Switch branches without committing the current branch. Stash current work: $ git stash Saving stashes with a message: $ git stash save \"<Stashing Message\" Check the stored stashes: $ git stash list Re-apply the changes that you just stashed $ git stash apply Track the stashes and their changes: $ git stash show Re-apply the previous commits: $ git stash pop Delete a most recent stash from the queue: $ git stash drop Delete all the available stashes at once: $ git stash clear Stash work on a separate branch: $ git stash branch <branch name> Git cherry pic Apply the changes introduced by some existing commit: $ git cherry-pick <commit id>","title":"11. Git stash"},{"location":"git_cheat_cmd/#12-git-merge","text":"Merge the branches: $ git merge <branch name> Merge the specified commit to currently active branch: $ git merge <commit> For instance: vscode > git clone url git checkout main branch git pull git checkout feature branch git merge main_branch ## if you want to update feature branch with main ## you will get merge conflicts in vscode ## resolve merge conflicts git commit ##check branch name and commit to your feature branch git push feature branch","title":"12. Git merge"},{"location":"git_cheat_cmd/#13git-rebase","text":"Apply a sequence of commits from distinct branches into a final commit. $ git rebase <branch name> Continue the rebasing process: $ git rebase-continue Abort the rebasing process: $ git rebase --skip Git interactive rebase Allow various operations like edit, rewrite, reorder, and more on existing commits. $ git rebase -i","title":"13.Git rebase"},{"location":"git_cheat_cmd/#14-git-remote","text":"Check the configuration of the remote server: $ git remote -v Add a remote for the repository: $ git remote add <short name> <remote URL> Fetch the data from remote server $ git fetch <Remote> Remove a remote connection from the repository: $ git remote rm <destination> Rename remote server: $ git remote rename <old name> <new name> Show additional information about a particular remote: $ git remote show <remote> Change remote: $ git remote set-url <remote name> <newURL>","title":"14. Git remote"},{"location":"git_cheat_cmd/#15-git-origin-master","text":"Push data to remote server: $ git push origin master Pull data from remote server: $ git pull origin master","title":"15. Git origin master"},{"location":"git_cheat_cmd/#16-pushing-updates","text":"Transfer the commits from your local Push data to remote server: $ git push origin master Force push data: $ git push <remote> <branch> -f pository to a remote server. Delete a remote branch by push command: $ git push origin -delete edited","title":"16. Pushing Updates"},{"location":"git_cheat_cmd/#17-pulling-updates","text":"Pull the data from the server: $ git pull origin master Pull a remote branch: $ git pull <remote branch URL>","title":"17. Pulling updates"},{"location":"git_cheat_cmd/#18-git-fetch","text":"Downloads branches and tags from one or more repositories. Fetch the remote repository: $ git fetch< repository Url> Fetch a specific branch: $ git fetch <branch URL><branch name> Fetch all the branches simultaneously: $ git fetch-all Synchronize the local repository: $ git fetch origin","title":"18. Git fetch"},{"location":"git_cheat_cmd/#19-undo-changes-git-revert","text":"Undo the changes $ git revert Revert a particular commit: $ git revert <commit-ish>","title":"19. Undo changes &gt; Git revert"},{"location":"git_cheat_cmd/#20-git-reset","text":"Reset the changes: $ git reset -hard $ git reset -soft $ git reset --mixed","title":"20. Git reset"},{"location":"k8-docs/","text":"Kubernetes From Scratch Kubernetes What is Kubernetes? Kubernetes is an open source orchestration tool developed by Google for managing micro- services or containerized applications across a distributed cluster of nodes. Kubernetes provides highly resilient infrastructure with zero downtime deployment capabilities, automatic rollback, scaling, and self-healing of containers (which consists of auto-placement, auto-restart, auto-replication, and scaling of containers on the basis of CPU usage). Kubernetes created from Borg & Omega projects by google as they use it to orchestrate they data center since 2003. Google open-sourced kubernetes at 2014. What is Orchestration Do? Configuring and scheduling of containers. Provisioning and deployments of containers. High Availability of containers. Configuration of the applications that run in containers. Scaling of containers to equally balance the application workloads across infrastructure. Allocation of HW resources between containers. Load balancing, traffic routing and service discovery of containers. Health monitoring of containers. Securing the interactions between containers. Famous Container Orchestrator Docker Swarm Mesos (Mesos Sphere) Normand Cloud Foundry Cattel Cloud (Azure, Amazon, Google, Alibaba, IBM) Kubernetes Components and Architecture K8s Master Node: the master server that will create the cluster and it has all the components and service that manage, plan, schedule and monitor all the worker nodes. Worker Node: the server that has host the applications as Pods and containers. Can make more than master server to make HA for the K8s components Kubernetes Master Node Components Below are the main components on the master node: API server \u2013 is the primary management components of kubernetes and is responsible for orchestrating all operations (scaling, updates, and so on) in the cluster. It also acts as the gateway to the cluster, so the API server must be accessible by clients from outside the cluster integration with CLI and GUI. Controller-manager - The Controller Manager is the engine that runs the core control loops, create Pods, watches the state of the cluster, and makes changes to drive status toward the desired state. Replication-Controller - A ReplicationController ensures that a specified number of pod replicas are running at any one time. It makes sure that a pod is always up and available. Node Controller - The node controller is a Kubernetes master component which manages various aspects of nodes. Scheduler - is identify the right node to place a container on based resource limitations or guarantees, taints, tolerations and affinity/anti-affinity roles. etcd cluster - etcd is a critical part of the Kubernetes. etcd database that stores the state of the cluster, including node and workload information in a key/value format. Add-ons: DNS: all Kubernetes clusters should have cluster DNS to resolve name of the containers inside master node as all the above components is containers inside master node Web UI: web-based UI for Kubernetes clusters. It allows users to manage and troubleshoot applications running in the cluster, as well as the cluster itself. Container runtime: The container runtime is the software that is responsible for running containers. Kubernetes supports several container runtimes: Docker , containerd , CRI-O Node (worker) components Below are the main components on a (worker) node: kubelet - the main service on a node, connect between Master and Node and ensuring that pods and their containers are healthy and running in the desired state. This component also reports to the master on the health of the host where it is running. kube-proxy - a proxy service that runs on each worker node to deal with individual host subnetting and expose services to the external world. It performs request forwarding to the correct pods/containers across the various isolated networks in a cluster. Kubectl kubectl command is a line tool that interacts with kube-apiserver and send commands to the master node. Each command is converted into an API call. Architecture Big Picture: Master Node responsible for the k8s and 4 workers have the Pods and containers) Kubernetes Concepts Making use of Kubernetes requires understanding the different abstractions it uses to represent the state of the system, such as services, pods, volumes, namespaces, and deployments. Pod - generally refers to one or more containers that should be controlled as a single application. Pods that run a single container. The \"one-container-per-Pod\" model is the most common Kubernetes use case; in this case, you can think of a Pod as a wrapper around a single container; Kubernetes manages Pods rather than managing the containers directly. Pods that run multiple containers that need to work together . A Pod can encapsulate an application composed of multiple co-located containers that are tightly coupled and need to share resources. These co-located containers form a single cohesive unit of service\u2014for example, one container serving data stored in a shared volume to the public, while a separate sidecar container refreshes or updates those files. The Pod wraps these containers, storage resources, and an ephemeral network identity together as a single unit. Note: Grouping multiple co-located and co-managed containers in a single Pod is a relatively advanced use case. You should use this pattern only in specific instances in which your containers are tightly coupled. Every Pod has 1 IP address and it has MAC address and we can allocate resource for it (CPUs, RAM, Network \u2026etc,). Communication between containers in different Pods via Pods IPs not container IP. Deployment \u2013 it provides declarative updates to applications, a deployment allows you to describe an application\u2019s life cycle, such as which images to use for the app, the number of pods there should be, and the way in which they should be updated. Deploy a replica set or pod Update pods and replica sets Rollback to previous deployment versions Scale a deployment Pause or continue a deployment Service \u2013 Allows you to dynamically access a group of replica Pods via IP and Port from your network and define name for the service. Pods are volatile, that is Kubernetes does not guarantee a given physical pod will be kept alive (for instance, the replication controller might kill and start a new set of pods). Instead, a service represents a logical set of pods and acts as a gateway, allowing (client) pods to send requests to the service without needing to keep track of which physical pods actually make up the service. Namespace \u2013 it\u2019s like resource pool in VMware or tenant in azure, it\u2019s a virtual cluster (a single physical cluster can run multiple virtual ones) Intended for environments with many users spread across multiple teams or projects, for isolation of concerns. Resources inside a namespace must be unique and cannot access resources in a different namespace. Also, a namespace can be allocated a resource quota to avoid consuming more than its share of the physical cluster\u2019s overall resources (CPU, RAM, and Security). Desired State - describes the desired state of a pod or a replica set, in a yaml file. The deployment controller then gradually updates the environment (for example, creating or deleting replicas or replicas image version upgrading or rollback via replication controller) until the current state matches the desired state specified in the deployment file. For example, if the yaml file defines 3 replicas for a pod but only two is currently running (current state), an extra one will get created. Note that replicas managed via a deployment should not be manipulated directly, only via new deployments. Secret - At the application level, Kubernetes secrets can store sensitive information (such as passwords, SSH keys, API keys or tokens) per cluster (a virtual cluster if using namespaces, physical otherwise). Kubernetes Secret can be injected into a Pod container either as an environment variable or mounted as a file. Using Kubernetes Secrets allows us to abstract sensitive data and configuration from application deployment. Note that secrets are accessible from any pod in the same cluster. Network policies for access to pods can be defined in a deployment. A network policy specifies how pods are allowed to communicate with each other and with other network endpoints. Note that storing sensitive data in a Kubernetes Secret does not make it secure. By default, all data in Kubernetes Secrets is stored as a plaintext encoded with base64. CoreDNS - CoreDNS is a flexible, extensible DNS server that can serve as the Kubernetes cluster DNS. Like Kubernetes, the CoreDNS project is hosted by the CNCF. You can use CoreDNS instead of kube-dns in your cluster by replacing kube-dns in an existing deployment, or by using tools like kubeadm that will deploy and upgrade the cluster for you Node-Proxy - a proxy service that runs on each worker node to deal with individual host subnetting and expose services to the external world. It performs request forwarding to the correct pods/containers across the various isolated networks in a cluster. Replica Set & Deployment - A ReplicaSet is a set of Pod templates that describes a set of Pod replicas. It uses a template that describes what each Pod must contain. The ReplicaSet ensures that a specified number of Pod replicas are running at any time. You can define a deployment to create a ReplicaSet or to remove deployments and adopt all their resources with new deployments. When you revise a deployment, a ReplicaSet is created that describes the state that you want. During a rollout, the deployment controller changes the actual state to the state that you want at a controlled rate. Each deployment revision can also be rolled back. Deployments can also be scaled. ReplicaSet is a part of Deployment. Daemon Set - Aggregating service logs, collecting node metrics, or running a networked storage cluster all require a container to be replicated across all nodes. In Kubernetes, this is done with a DaemonSet. A DaemonSet ensures that an instance of a specific pod is running on all (or a selection of) nodes in a cluster. This page gathers resources on how to use and deploying a daemon to all nodes. DaemonSet are used to ensure that some or all of your K8S nodes run a copy of a pod, which allows you to run a daemon on every node. Why use DaemonSets? To run a daemon for cluster storage on each node , such as: glusterd To run a daemon for logs collection on each node, such as: logstash To run a daemon for node monitoring on ever note, such as: collectd Label - Labels are key/value pairs that are attached to Kubernetes objects, such as pods (this is usually done indirectly via deployments). Labels are intended to be used to specify identifying attributes of objects that are meaningful and relevant to users. Labels can be used to organize and to select subsets of objects. Some labels are required for every deployment resource: (application, version, release, stage) can add Owner Contain identifying information and are a used by selector queries or within selector sections in object definitions. Example deployment metadata: Let\u2019s create a pod that initially has one label (stage=production): kubectl apply -f https://raw.githubusercontent.com/openshift- evangelists/kbe/main/specs/labels/pod.yaml kubectl get pods --show-labels NAME READY STATUS RESTARTS AGE LABELS labelex 1/1 Running 0 10m stage=production In above get pods command note the --show-labels option that output the labels of an object in an additional column. You can add a label to the pod as: kubectl label pods labelex owner= Ahmed kubectl get pods --show-labels NAME READY STATUS RESTARTS AGE LABELS labelex 1/1 Running 0 16m stage=production,owner= Ahmed Selector - let me apply policy on labels To use a label for filtering and apply policy, for example to list only pods that have an owner that equals Ahmed, use the --selector option: kubectl get pods --selector owner=Ahmed NAME READY STATUS RESTARTS AGE labelex 1/1 Running 0 27m The --selector option can be abbreviated to -l, so to select pods that are labelled with stage= production, do: kubectl get pods -l stage=production NAME READY STATUS RESTARTS AGE labelex 1/1 Running 0 27m Now, let\u2019s list all pods that are either labelled with stage=development or with stage =production: $ kubectl get pods -l stage in (production, development)' NAME READY STATUS RESTARTS AGE labelex 1/1 Running 0 43m labelexother 1/1 Running 0 3m Annotations - Are used for non-identifying information. Stuff not used internally by k8s . You can\u2019t specify selectors over them within Kubernetes, but they can be used by external tools and libraries. As the internal performance of Kubernetes is not negatively impacted by huge annotations, the keys and values are not constrained like labels. Affinity, Anti Affinity - nodeSelector provides a very simple way to constrain pods to nodes with particular labels. The affinity/anti-affinity feature, greatly expands the types of constraints you can express. The key enhancements are The affinity/anti-affinity language is more expressive. The language offers more matching rules besides exact matches created with a logical AND operation; You can indicate that the rule is \"soft\"/\"preference\" rather than a hard requirement, so if the scheduler can't satisfy it, the pod will still be scheduled; You can constrain against labels on other pods running on the node (or other topological domain), rather than against labels on the node itself, which allows rules about which pods can and cannot be co-located. The affinity feature consists of two types of affinity, \" node affinity \" and \" inter- pod affinity/anti-affinity \". Node affinity is like the existing nodeSelector (but with the first two benefits listed above), while inter-pod affinity/anti-affinity constrains against pod labels rather than node labels, as described in the third item listed above, in addition to having the first and second properties listed above. Taints, Tolerations - Taints are used to repel Pods from specific nodes. This is quite similar to the node anti-affinity, however, taints and tolerations take a slightly different approach. Instead of applying the label to a node, we apply a taint that tells a scheduler to repel Pods from this node if it does not match the taint. Only those Pods that have a toleration for the taint can be let into the node with that taint. Tolerations are applied to pods, and allow (but do not require) the pods to schedule onto nodes with matching taints. Taints and tolerations work together to ensure that pods are not scheduled onto inappropriate nodes. One or more taints are applied to a node; this marks that the node should not accept any pods that do not tolerate the taints. You add a taint to a node using kubectl taint. For example, # kubectl taint nodes node1 key=value:NoSchedule Above example places a taint on node node 1. The taint has key key, value value, and taint effect NoSchedule. This means that no pod will be able to schedule onto node1 unless it has a matching toleration. To remove the taint added by the command above, you can run: # kubectl taint nodes node1 key:NoSchedule- Use Cases for Taints and Toleration hardware s: Dedicated node, Nodes with special Config-map - ConfigMaps bind configuration files, command-line arguments, environment variables, port numbers, and other configuration artifacts to your Pods' containers and system components at runtime. ConfigMaps enable you to separate your configurations from your Pods and components, which helps keep your workloads portable. Instead of repeat Pods configurations in yaml file for each Pod, We can let the yaml file read the configuration from ConfigMap file. Volume - similar to a container volume in Docker, but a Kubernetes volume applies to a whole pod and is mounted on all containers in the pod. Kubernetes guarantees data is preserved across container restarts. The volume will be removed only when the pod gets destroyed. Also, a pod can have multiple volumes (possibly of different types) associated. Kubernetes commands Overview of kubectl The kubectl command line tool lets you control Kubernetes clusters. For configuration, kubectl looks for a file named config in the $HOME/.kube directory. You can specify other kubeconfig files by setting the KUBECONFIG environment variable or by setting the --kubeconfig flag. Syntax kubectl [command] [TYPE] [NAME] [flags] Where command, TYPE, NAME, and flags are: Command: Specifies the operation that you want to perform on one or more resources, for example create, get, describe, and delete. TYPE: Specifies the resource type. Resource types are case-insensitive and you can specify the singular, plural, or abbreviated forms. For example, the following commands produce the same output: kubectl get pod pod1 kubectl get pods pod1 kubectl get po pod1 NAME: Specifies the name of the resource. Names are case-sensitive. If the name is omitted, details for all resources are displayed, for example kubectl get pods. When performing an operation on multiple resources, you can specify each resource by type and name or specify one or more files: To specify resources by type and name: To group resources if they are all the same type: TYPE1 name1 name2 name<#>. Example: kubectl get pod example-pod1 example-pod2 To specify multiple resource types individually: TYPE1/name1 TYPE1/name2 TYPE2/name3 TYPE<#>/name<#>. Example: kubectl get pod/example-pod1 replicationcontroller/example- rc1 To specify resources with one or more files: -f file1 -f file2 -f file<#> Use YAML rather than JSON since YAML tends to be more user-friendly, especially for configuration files. Example: kubectl get pod -f ./pod.yaml flags: Specifies optional flags. For example, you can use the -s or --server flags to specify the address and port of the Kubernetes API server. Caution: Flags that you specify from the command line override default values and any corresponding environment variables. If you need help, just run kubectl help from the terminal window. Top Commands #Kubernetes Commands Helper setup to edit .yaml files with Vim: VIM Setup for Yaml files List of general purpose commands for Kubernetes management: PODS Create Deployments Scaling PODs POD Upgrade / History Services Volumes Secrets ConfigMaps Ingress Horizontal Pod Autoscalers Scheduler Taints and Tolerations Troubleshooting Role Based Access Control (RBAC) Security Contexts Pod Security Policies Network Policies VIM Setup for Yaml files Put the following lines in ~/.vimrc: Yaml file handling autocmd FileType yaml setlocal ts=2 sts=2 sw=2 expandtab filetype plugin indent on autocmd FileType yaml setl indentkeys-=<:> Copy paste with ctr+c, ctr+v, etc :behave mswin :set clipboard=unnamedplus :smap \"_d :smap y :smap x :imap pi :smap p :smap 1> :smap 1< Keyboard hints: ctrl + f: auto indent line (requires INSERT mode) PODS $ kubectl get pods $ kubectl get pods --all-namespaces $ kubectl get pod monkey -o wide $ kubectl get pod monkey -o yaml $ kubectl describe pod monkey Create Deployments Create single deployment: $ kubectl run nginx --image=nginx \u2013record Scaling PODs $ kubectl scale deployment/POD_NAME --replicas=N POD Upgrade and history List history of deployments $ kubectl rollout history deployment/DEPLOYMENT_NAME Jump to specific revision $ kubectl rollout undo deployment/DEPLOYMENT_NAME --to-revision=N Services List services $ kubectl get services Expose PODs as services (creates endpoints) $ kubectl expose deployment nginx --port=80 --type=NodePort Volumes Lits Persistent Volumes and Persistent Volumes Claims: $ kubectl get pv $ kubectl get pvc Secrets $ kubectl get secrets $ kubectl create secret generic --help $ kubectl create secret generic mysql --from-literal=password=root $ kubectl get secrets mysql -o yaml ConfigMaps $ kubectl create configmap foobar --from-file=config.js $ kubectl get configmap foobar -o yaml DNS List DNS-PODs: $ kubectl get pods --all-namespaces |grep dns Check DNS for pod nginx (assuming a busybox POD/container is running) $ kubectl exec -ti busybox -- nslookup nginx Note: kube-proxy running in the worker nodes manage services and set iptables rules to direct traffic. Ingress Commands to manage Ingress for ClusterIP service type: $ kubectl get ingress $ kubectl expose deployment ghost --port=2368 Spec for ingress: backend Horizontal Pod Autoscaler When heapster runs: $ kubectl get hpa $ kubectl autoscale --help DaemonSets $ kubectl get daemonsets $ kubectl get ds Scheduler NodeSelector based policy: $ kubectl label node minikube foo=bar Node Binding through API Server: $ kubectl proxy $ curl -H \"Content-Type: application/json\" -X POST --data @binding.json http://localhost:8001/api/v1/namespaces/default/pods/foobar-sched/binding Tains and Tolerations $ kubectl taint node master foo=bar:NoSchedule Troubleshooting $ kubectl describe $ kubectl logs $ kubectl exec $ kubectl get nodes --show-labels $ kubectl get events Docs Cluster: https://kubernetes.io/docs/tasks/debug-application-cluster/debug-cluster/ https://github.com/kubernetes/kubernetes/wiki/Debugging-FAQ Role Based Access Control Role ClusterRule Binding ClusterRoleBinding $ kubectl create role fluent-reader --verb=get --verb=list --verb=watch --resource=pods $ kubectl create rolebinding foo --role=fluent-reader --user=minikube $ kubectl get rolebinding foo -o yaml Security Contexts Docs: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/ spec securityCOntext runAsNonRoot: true Pod Security Policies Docs: https://github.com/kubernetes/kubernetes/blob/master/examples/podsecuritypolicy/rbac/READ ME.md Network Policies Network isolation at Pod level by using annotations $ kubectl annotate ns \"net.beta.kubernetes.io/network-policy={\\\"ingress\\\": {\\\"isolation\\\": \\\"DefaultDeny\\\"}}\" More about Network Policies as a resource: https://kubernetes.io/docs/tasks/administer-cluster/declare-network-policy/ Kubeadm Kubeadm is a tool built to provide kubeadm init and kubeadm join as best-practice \u201cfast paths\u201d for creating Kubernetes clusters. Kubeadm performs the actions necessary to get a minimum viable cluster up and running. By design, it cares only about bootstrapping, not about provisioning machines. Likewise, installing various nice-to-have addons, like the Kubernetes Dashboard, monitoring solutions, and cloud- specific addons, is not in scope. Instead, we expect higher-level and more tailored tooling to be built on top of kubeadm, and ideally, using kubeadm as the basis of all deployments will make it easier to create conformant clusters. Kubernetes YAML As stated on the Wikipedia page for JSON, YAML (Yet Another Markup Language) is a superset of JSON, which means that it has all the functionality of JSON, but it also extends this functionality to some degree. YAML is basically a wrapper around JSON, doing everything that JSON can do and then some. YAML VS JSON The YAML file takes less space than the JSON file. YAML requires less characters than JSON does. YAML allows for comments, while JSON doesn\u2019t. YAML Syntax YAML files consist of maps (or dictionaries) of key-value pairs. A YAML map is simply an object, containing keys and values. Here\u2019s a map of three key-value pairs: key1: value1 key2: value2 key3: value3 A single key can itself contain a map: key1: subkey1: subvalue1 subkey2: subvalue2 subkey3: subvalue3 YAML also has lists (are just an array of values for a particular key): list: item1 item2 item3 Lists can also contain maps: list: item1 mapItem1: value mapItem2: value Kubernetes YAML Basics There are a few required fields in every Kubernetes YAML file to work: apiVersion - Which version of the Kubernetes API you're using to create this object Kind - What kind of object you want to create metadata - Data that helps uniquely identify the object, including a name string, UID, and optional namespace spec - What state you desire for the object Let\u2019s take another look at our deployment.yaml apiVersion: v1 kind: pod metadata: name: website labels: name: web spec: containers: - name: web-server image: nginx resources: limits: memory: \"128Mi\" cpu: \"500m\" ports: - containerPort: 80 apiVersions Which apiVersion should I use? Kind apiVersion CertificateSigningRequest certificates.k8s.io/v1beta1 ClusterRoleBinding rbac.authorization.k8s.io/v1 ClusterRole rbac.authorization.k8s.io/v1 ComponentStatus v1 ConfigMap v1 ControllerRevision apps/v1 CronJob batch/v1beta1 DaemonSet extensions/v1beta1 Deployment extensions/v1beta1 Endpoints v1 Event v1 HorizontalPodAutoscaler autoscaling/v1 Ingress extensions/v1beta1 Job batch/v1 LimitRange v1 Namespace v1 NetworkPolicy extensions/v1beta1 Node v1 PersistentVolumeClaim v1 PersistentVolume v1 PodDisruptionBudget policy/v1beta1 Pod v1 PodSecurityPolicy extensions/v1beta1 PodTemplate v1 ReplicaSet extensions/v1beta1 ReplicationController v1 ResourceQuota v1 RoleBinding rbac.authorization.k8s.io/v1 Role rbac.authorization.k8s.io/v1 Secret v1 ServiceAccount v1 Service v1 StatefulSet apps/v1 v1 This was the first stable release of the Kubernetes API. It contains many core objects. apps/v1 apps is the most common API group in Kubernetes, with many core objects being drawn from it and v1. It includes functionality related to running applications on Kubernetes, like Deployments, RollingUpdates, and ReplicaSets. autoscaling/v1 This API version allows pods to be autoscaled based on different resource usage metrics. This stable version includes support for only CPU scaling, but future alpha and beta versions will allow you to scale based on memory usage and custom metrics. batch/v1 The batch API group contains objects related to batch processing and job-like tasks (rather than application-like tasks like running a webserver indefinitely). This apiVersion is the first stable release of these API objects. batch/v1beta1 A beta release of new functionality for batch objects in Kubernetes, notably including CronJobs that let you run Jobs at a specific time or periodicity. apiVersion: v1 kind: Pod metadata: name: rss-site labels: app: web When I write that apiversion in YAML file : group/v APIVersions Reference link https://kubernetes.io/docs/reference/generated/kubernetes- api/v1.18/#-strong-api-overview-strong- Kind What kindly of object you want to create: To check all kinds available to create: kubectl api-resources Hints: vsCode is an excellent tool to make the YAML file. You can use Helm or Bitnami as chart and ready apps You can use container platform and service catalog. Kubectl Autocomplete source <(kubectl completion bash) # setup autocomplete in bash into the current shell, bash-completion package should be installed first. echo \"source <(kubectl completion bash)\" >> ~/.bashrc # add autocomplete permanently to your bash shell. Install kubernetes Step 1 - Kubernetes Installation The three-node cluster that we will be forming in this example will consist of a Master node and a Two Slave nodes, therefore, follow the steps described below to install Kubernetes on the CentOS nodes. Kubernetes will be installed as a container so we must install Docker engine All nodes need to have Kubernetes installed on them. We will prepare all servers for Kubernetes installation by changing the existing configuration on servers, and also installing some packages, including docker-ce and kubernetes itself. Configure Hosts File vim /etc/hosts Add all servers IPs master and workers to the hosts file below. 192.168.179.133 k8s-master 192.168.179.131 node01 192.168.179.132 node02 Save and exit. -Configure Firewall The nodes, containers, and pods need to be able to communicate across the cluster to perform their functions. Firewalld is enabled in CentOS by default on the front-end. Add the following ports by entering the listed commands. On the Master Node enter: firewall-cmd --permanent --add-port=6443/tcp firewall-cmd --permanent --add-port=2379-2380/tcp firewall-cmd --permanent --add-port=10250/tcp firewall-cmd --permanent --add-port=10251/tcp firewall-cmd --permanent --add-port=10252/tcp firewall-cmd --permanent --add-port=10255/tcp firewall-cmd --reload Enter the following commands on each worker node: firewall-cmd --permanent --add-port=10251/tcp firewall-cmd --permanent --add-port=10252/tcp firewall-cmd \u2013reload To list all firewall rules firewall-cmd --list-all Update Iptables Settings Set the net.bridge.bridge-nf-call-iptables to \u20181\u2019 in your sysctl config file. This ensures that packets are properly processed by IP tables during filtering and port forwarding. cat < /etc/sysctl.d/k8s.conf net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 EOF sysctl --system Enable br_netfilter Kernel Module The br_netfilter module is required for kubernetes installation. Enable this kernel module so that the packets traversing the bridge are processed by iptables for filtering and for port forwarding, and the kubernetes pods across the cluster can communicate with each other. Run the command below to enable the br_netfilter kernel module. modprobe br_netfilter echo '1' > /proc/sys/net/bridge/bridge-nf-call-iptables -Disable SELinux sudo setenforce 0 sudo sed -i \u2018s/^SELINUX=enforcing$/SELINUX=permissive/\u2019 /etc/selinux/config Disable SWAP If swap is not disabled, kubelet service will not start on the masters and nodes, for Platform9 Managed Kubernetes version 3.3 and above. swapoff \u2013a And then edit the '/etc/fstab' file. Comment the swap line UUID as below. **Install Docker on the nodes ** Install the package dependencies for docker-ce. yum install -y yum-utils device-mapper-persistent-data Add the docker repository to the system and install docker-ce using the yum command. yum-config-manager --add-repo https://download.docker.com/linux/centos/docker- ce.repo yum install -y docker-ce Enable Docker on the nodes Enable and start the Docker utility on both the nodes by running the following command on each: systemctl enable docker systemctl start docker Add the Kubernetes signing key on the nodes Add the kubernetes repository and key to the centos 7 system by running the following command. cat < /etc/yum.repos.d/kubernetes.repo [kubernetes] name=Kubernetes baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64 enabled=1 gpgcheck=1 repo_gpgcheck=1 gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg EOF Install Kubeadm Now install the kubernetes packages kubeadm, kubelet, and kubectl using the yum command below. yum install -y kubelet kubeadm kubectl You can check the version number of Kubeadm and also verify the installation through the following command: kubeadm version After the installation is complete, restart all those servers. Log in again to the server and start the services, docker and kubelet. systemctl start docker && systemctl enable docker systemctl start kubelet && systemctl enable kubelet Change the cgroup-driver We need to make sure the docker-ce and kubernetes are using same 'cgroup'. Check docker cgroup using the docker info command. docker info | grep -i cgroup And you see the docker is using 'cgroupfs' as a cgroup-driver. Now run the command below to change the kuberetes cgroup-driver to 'cgroupfs'. sed -i 's/cgroup-driver=systemd/cgroup-driver=cgroupfs/g' /etc/systemd/system/kubelet.service.d/10-kubeadm.conf Step 2 - Kubernetes Cluster Initialization In this step, we will initialize the kubernetes master cluster configuration. Move the shell to the master server 'master' and run the command below to set up the kubernetes master. kubeadm init --apiserver-advertise-address=10.0.15.10 --pod-network- cidr=10.244.0.0/16 --apiserver-advertise-address = determines which IP address Kubernetes should advertise its API server on. --pod-network-cidr = specify the range of IP addresses for the pod network. We're using the 'flannel' virtual network. If you want to use another pod network such as weave-net or calico, change the range IP address. When the Kubernetes initialization is complete, you will get the result as below. Copy the ' kubeadm join ... ... ... ' command to your text editor. The command will be used to register new nodes to the kubernetes cluster. Now in order to use Kubernetes, we need to run some commands as on the result. Create new '.kube' configuration directory and copy the configuration 'admin.conf'. mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config Next, deploy the flannel network to the kubernetes cluster using the kubectl command. kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube- flannel.yml The flannel network has been deployed to the Kubernetes cluster. Wait for a minute and then check kubernetes node and pods using commands below. kubectl get nodes kubectl get pods --all-namespaces And you will get the \u2018master' node is running as a 'master' cluster with status 'ready', and you will get all pods that are needed for the cluster, including the 'kube-flannel-ds' for network pod configuration. Make sure all kube-system pods status is 'running'. Kubernetes cluster master initialization and configuration has been completed. Step 3 - Adding node1 and node2 to the Cluster In this step, we will add node1 and node2 to join the 'master' cluster. Connect to the node1 server and run the kubeadm join command as we copied on the top. kubeadm join 192.168.179.133:6443 --token l12jxw.3c960ner9320dpg8 --discovery- token-ca-cert-hash sha256:af54557ca10c0702f6e29bf4d7d96a76eea38e8c85a1bc5cfd6d56ee9d9a1dd7 Connect to the node2 server and run the kubeadm join command as we copied on the top. Wait for some minutes and back to the \u2018master' master cluster server check the nodes and pods using the following command. kubectl get nodes kubectl get pods --all-namespaces Node1 and Node2 have been added to the kubernetes cluster. Step 4 - Create First Pod In this step, we will do a test by deploying the Nginx pod to the kubernetes cluster. Configure the yaml file of deployment and if want to publish it from external network must to configure service yaml file Pod yaml apiVersion: v1 kind: Pod metadata: name: website labels: name: web spec: containers: name: web-server image: nginx resources: limits: memory: \"128Mi\" cpu: \"500m\" ports: containerPort: 80 Service yaml apiVersion: v1 kind: Service metadata: name: Publish-website spec: selector: name: web type: NodePort ports: name: http port: 80 targetPort: 80 nodePort: 30003 protocol: TCP Above example selector name is the label name on the pod deployment yaml file Publish containers from nodeport type use ports from 30000 to 32000. Create new deployment named 'nginx' using the kubectl command. kubectl create -f website.yaml To see details of the 'nginx' deployment sepcification, run the following command. kubectl describe deployment nginx Next, we will expose the nginx pod accessible via the internet. And we need to create new service NodePort for this. kubectl create -f service.yaml If I want to change anything in yaml configuration and reapply it use command. kubectl apply -f service.yaml Make sure there is no error. Now check the nginx service nodeport and IP using the kubectl command below. kubectl get pods kubectl get svc kubectl describe svc Now you will get the nginx pod is now running under cluster IP address \u201810.109.154.222\u2019 port 80, and the node main IP address '192.168.179.131' on port '30003'. The Nginx Pod has now been deployed under the Kubernetes cluster and it's accessible via the internet, now access from the web browser. http://192.168.179.131:30003/ Kubernetes Namespace **What is a Namespace? ** You can think of a Namespace as a virtual cluster inside your Kubernetes cluster. You can have multiple namespaces inside a single Kubernetes cluster, and they are all logically isolated from each other. They can help you and your teams with organization, security, and even performance!** In most Kubernetes distributions, the cluster comes out of the box with a Namespace called \u201cdefault.\u201d In fact, there are actually three namespaces that Kubernetes ships with: default, kube- system (used for Kubernetes components), and kube-public (used for public resources). Kube- public isn\u2019t really used for much right now, and it\u2019s usually a good idea to leave kube-system alone Functionality of Namespace Following are some of the important functionalities of a Namespace in Kubernetes \u2212 Namespaces help pod-to-pod communication using the same namespace. Namespaces are virtual clusters that can sit on top of the same physical cluster. They provide logical separation between the teams and their environments. Create a Namespace The following command is used to create a namespace. apiVersion: v1 kind: Namespace metadata name: MicroFinance Control the Namespace The following command is used to control the namespace. kubectl create \u2013f namespace.yml ---------> 1 kubectl get namespace -----------------> 2 kubectl get namespace ------->3 kubectl describe namespace ---->4 kubectl delete namespace In the above code, We are using the command to create a namespace. This will list all the available namespace. This will get a particular namespace whose name is specified in the command. This will describe the complete details about the service. This will delete a particular namespace present in the cluster. This is a sample resource quota limit for namespace apiVersion: v1 kind: ResourceQouta metadata: name: compute-qouta namespace: prod spec: hard: pods: \"10\" requests.cpu: \"4\" requests.memory: 5Gi limits.cpu: \"10\" limits.memory: 10Gi This is the output after apply limits on namespace kubectl describe namespace prod Using Namespace in Service - Example Following is an example of a sample file for using namespace in service. apiVersion: v1 kind: Service metadata: name: Micro namespace: MicroFinance labels: component: Micro spec: type: LoadBalancer selector: component: Micro ports: name: http port: 9200 protocol: TCP name: transport port: 9300 protocol: TCP In the above code, we are using the same namespace under service metadata with the name of MicroFinance. Kubernetes Deployments Deployments are upgraded and higher version of replication controller. They manage the deployment of replica sets which is also an upgraded version of the replication controller. They have the capability to update the replica set and are also capable of rolling back to the previous version. They provide many updated features of matchLabels and selectors . We have got a new controller in the Kubernetes master called the deployment controller which makes it happen. It has the capability to change the deployment midway. Changing the Deployment Updating \u2212 The user can update the ongoing deployment before it is completed. In this, the existing deployment will be settled and new deployment will be created. Deleting \u2212 The user can pause/cancel the deployment by deleting it before it is completed. Recreating the same deployment will resume it. Rollback \u2212 We can roll back the deployment or the deployment in progress. The user can create or update the deployment by using DeploymentSpec.PodTemplateSpec = oldRC.PodTemplateSpec. Deployment Strategies Deployment strategies help in defining how the new RC should replace the existing RC. Recreate \u2212 This feature will kill all the existing RC and then bring up the new ones. This results in quick deployment however it will result in downtime when the old pods are down and the new pods have not come up. Rolling Update \u2212 This feature gradually brings down the old RC and brings up the new one. This results in slow deployment, however there is no deployment. At all times, few old pods and few new pods are available in this process. The configuration file of Deployment looks like this. apiVersion: extensions/v1beta1 --------------------->1 kind: Deployment --------------------------> 2 metadata: name: Tomcat-ReplicaSet spec: replicas: 3 template: metadata: lables: app: Tomcat-ReplicaSet tier: Backend spec: containers: name: Tomcatimage: tomcat: 8.0 ports: containerPort: 7474 In the above code, the only thing which is different from the replica set is we have defined the kind as deployment. **Create Deployment ** kubectl create \u2013f Deployment.yaml \u2013record -\u2013record to see the deployments by command # kubectl rollout status deployment/Deployment Deployment \"Deployment\" created successfully. Fetch the Deployment kubectl get deployments kubectl get deploy -o wide NAME DESIRED CURRENT UP-TO-DATE AVILABLE AGE Deployment 3 3 3 3 20s Check the Status of Deployment kubectl rollout status deployment/Deployment Updating the Deployment kubectl apply \u2013f Deployment.yaml (after update the new version inside the yaml file) kubectl set image deployment/Deployment tomcat=tomcat:6.0 Rolling Back to Previous Deployment kubectl rollout undo deployment/Deployment \u2013to-revision=2 Check rollout history kubectl rollout history deployment/Deployment Get ReplicaSet kubectl get replicaset [root@master ~]# kubectl get replicaset NAME DESIRED CURRENT READY AGE Tomcat-ReplicaSet 3 3 3 2d14h Delete ReplicaSet (it will also delete all underlying Pods) kubectl delete ReplicaSet < ReplicaSet name> [root@master ~]# delete replicaset Tomcat-ReplicaSet Scaling the Deployment kubectl scale --replicas=6 \u2013f Deployment.yaml To deploy from local image include imagePullPolicy: Never in the deployment yaml file spec: containers: name: nginx-web image: nginx resources: limits: memory: \"100Mi\" cpu: \"100m\" imagePullPolicy: Never Assign Pods to Nodes 1- Assign label to your nodes List the nodes in your cluster, along with their labels: kubectl get nodes --show-labels Chose one of your nodes, and add a label to it: kubectl label nodes centos type=Master Verify that your chosen node has a type=Master label: 2- In the YAML file add selector of nodeSelector with the label spec: containers: name: nginx image: nginx nodeSelector: type: Master Apply your deployment kubectl apply -f Deployment.yaml To get into Pod Shell kubectl exec --stdin --tty -- /bin/bash [root@master ~]# kubectl exec --stdin --tty pod/nginx-deployment-597fbc7d6f- jhwf2 -- /bin/bash root@nginx-deployment-597fbc7d6f-jhwf2:/# ls Kubernetes Networking Networking in any architecture or technology is very essential to understand if you want to deploy the applications over the network and also understanding how the network works that will help you to troubleshoot if you have any network issue. So we must know exactly what is happening in the network and how all the connections are establishing for the communication over the network. This is a basic kubernetes architecture where you have some worker nodes, and few pods are running on it with couple of containers. When you have these many components, how all these components are establishing its connection to make the application accessible over the network? The answer is that happens only through kubernetes networking. Types of Kubernetes Networking Container to Container Communication: When you have one or more containers within a pod that shares the same host networking. So pods will get its own IP address, all container shares same ip address but it works on different port. Communication between containers happens within the pod itself on different port. So all containers will be able to communicate each other by default. Pod to Pod communication: As said earlier, each pods will get its own ip address. There are sub types within Pod to Pod communication, that is. Intra-node Pod Network - Communication of pods running on a single node. Inter-node Pod Network - Communication of pods running in different nodes. On the first case, each pod running on single worker node will have the communication by default, because all ip address of pods will be different and assigned from your local network. Since it shares the same host. On the second case, when you have pod running on multiple worker nodes, communication between these pods happens through network plugin that will create some route tables. It forwards the traffic from any pod to any destination pods. Pod to Service Communication: Service is kubernetes resource type that expose our application to outside the cluster. Through which pod can send the traffic to services. 4 External to Service Communication: In order to access our application from outside the cluster, external traffic should be allowed to reach the server within the cluster... This can be achieved using these different types. ClusterIP NodePort LoadBalancer Extername Each types has its own function and purpose. Cluster IP - It is the default kubernetes service used for internal communication within the cluster. NodePort - It will open a ports on each nodes and traffic will be forwarded to the service through random port. And I can access the service (Pod) with the node IP + Defined port. LoadBalancer - It is a type that forwards all external traffic to a service through this type. And I can access the service (Pod) with the node name only. External Name - it is a type used to access a service internally that is hosted outside cluster through DNS CName or A record... LoadBalancer service is not in kubernetes by default, to use it we must create ingress network. Ingress Policy Ingress exposes HTTP and HTTPS routes from outside the cluster to services within the cluster. Traffic routing is controlled by rules defined on the Ingress resource. internet | [ Ingress ] --|-----|-- [ Services ] An Ingress may be configured to give Services externally-reachable URLs, load balance traffic, terminate SSL / TLS, and offer name based virtual hosting. An Ingress controller is responsible for fulfilling the Ingress, usually with a load balancer, though it may also configure your edge router or additional frontends to help handle the traffic. An Ingress does not expose arbitrary ports or protocols. Exposing services other than HTTP and HTTPS to the internet typically uses a service of type Service.Type=NodePort or Service.Type=LoadBalancer . Prerequisites You must have an ingress controller to satisfy an Ingress. Only creating an Ingress resource has no effect. You may need to deploy an Ingress controller such as ingress-nginx. The Ingress Resource A minimal Ingress resource example: apiVersion: extensions/v1beta1 kind: Ingress metadata: name: test-ingress namespace: critical-space annotations: nginx.ingress.kubernetes.io/rewrite-target: / spec: rules: http: paths: path: /website backend: serviceName: website-service servicePort: 80 Then I make deployment with type LoadBalancer apiVersion: v1 kind: Service metadata: name: mywebsite spec: ports: name: http port: 80 protocol: TCP targetPort: 80 type: LoadBalancer selector: Ingress and egress The bulk of securing network traffic typically revolves around defining egress and ingress rules. From the point of view of a Kubernetes pod, ingress is incoming traffic to the pod, and egress is outgoing traffic from the pod. In Kubernetes network policy, you create ingress and egress \u201callow\u201d rules independently (egress, ingress, or both). Kubernetes NodePort vs LoadBalancer vs Ingress? When should I use what? They are all different ways to get external traffic into your cluster ClusterIP A ClusterIP service is the default Kubernetes service. It gives you a service inside your cluster that other apps inside your cluster can access. There is no external access. The YAML for a ClusterIP service looks like this: apiVersion: v1 kind: Service metadata: name: my-internal-service spec: selector: app: my-app type: ClusterIP ports: name: http port: 80 targetPort: 80 protocol: TCP If you can\u2019t access a ClusterIP service from the internet, Turns out you can access it using the Kubernetes proxy! Start the Kubernetes Proxy: kubectl proxy --port=8080 Now, you can navigate through the Kubernetes API to access this service using this scheme: http://localhost:8080/api/v1/proxy/namespaces/ /services/ : / So to access the service we defined above, you could use the following address: http://localhost:8080/api/v1/proxy/namespaces/default/services/my-internal-service:http/ When would you use this? There are a few scenarios where you would use the Kubernetes proxy to access your services. Debugging your services, or connecting to them directly from your laptop for some reason Allowing internal traffic, displaying internal dashboards, etc. Because this method requires you to run kubectl as an authenticated user, you should NOT use this to expose your service to the internet or use it for production services. NodePort A NodePort service is the most primitive way to get external traffic directly to your service. NodePort, as the name implies, opens a specific port on all the Nodes (the VMs), and any traffic that is sent to this port is forwarded to the service. The YAML for a NodePort service looks like this: apiVersion: v1 kind: Service metadata: name: my-nodeport-service spec: selector: app: my-app type: NodePort ports: name: http port: 80 targetPort: 80 nodePort: 30036 protocol: TCP NodePort service has two differences from a normal \u201cClusterIP\u201d service. First, the type is \u201cNodePort.\u201d There is also an additional port called the NodePort that specifies which port to open on the nodes. If you don\u2019t specify this port, it will pick a random port. Most of the time you should let Kubernetes choose the port; there are many caveats to what ports are available for you to use. When would you use this? There are many downsides to this method: You can only have one service per port You can only use ports 30000\u201332767 If your Node/VM IP address change, you need to deal with that For these reasons, I don\u2019t recommend using this method in production to directly expose your service. If you are running a service that doesn\u2019t have to be always available, or you are very cost sensitive, this method will work for you. A good example of such an application is a demo app or something temporary. **LoadBalancer ** A LoadBalancer service is the standard way to expose a service to the internet. This will spin up a Network Load Balancer that will give you a single IP address that will forward all traffic to your service. When would you use this? If you want to directly expose a service, this is the default method. All traffic on the port you specify will be forwarded to the service. There is no filtering, no routing, etc. This means you can send almost any kind of traffic to it, like HTTP, TCP, UDP, Websockets, gRPC, or whatever. The big downside is that each service you expose with a LoadBalancer will get its own IP address, and you have to pay for a LoadBalancer per exposed service, which can get expensive! Ingress Unlike all the above examples, Ingress is actually NOT a type of service. Instead, it sits in front of multiple services and act as a \u201csmart router\u201d or entry point into your cluster. You can do a lot of different things with an Ingress, and there are many types of Ingress controllers that have different capabilities. The YAML for an Ingress object with a L7 HTTP Load Balancer might look like this: apiVersion: extensions/v1beta1 kind: Ingress metadata: name: my-ingress spec: backend: serviceName: other servicePort: 8080 rules: host: foo.mydomain.com http: paths: backend: serviceName: foo servicePort: 8080 host: mydomain.com http: paths: path: /bar/* backend: serviceName: bar servicePort: 8080 When would you use this? Ingress is probably the most powerful way to expose your services, but can also be the most complicated. There are many types of Ingress controllers, from the Google Cloud Load Balancer, Nginx, Contour, Istio, and more. There are also plugins for Ingress controllers, like the cert- manager, that can automatically provision SSL certificates for your services. Ingress is the most useful if you want to expose multiple services under the same IP address, and these services all use the same L7 protocol (typically HTTP). You only pay for one load balancer if you are using the native GCP integration, and because Ingress is \u201csmart\u201d you can get a lot of features out of the box (like SSL, Auth, Routing, etc). Kubernetes Storage Persistent volume claim (PVC) A Persistent Volume Claim (PVC) is a claim request for some storage space by users. Persistent volume (PV) A Persistent Volume (PV) is a piece of storage in the cluster that has been provisioned by an administrator or dynamically provisioned using Storage Classes. In simple words, Persistent Volume is a solution to store data of our containers permanently even after the pods got deleted. How it is different from other kubernetes volume types? Let\u2019s say you have multiple pod running on different nodes and you used hostpath volume type. Your data written by pod 1 running on worker node 1 will be resides only on worker node 1 and that cannot be access by pod 2 running on worker node 2. Similarly pod 1 cannot access data written by pod 2 on worker node 2. Right? Since hostpath is a type that writes data only on your local node directory. It\u2019s not kind of a shared volume. Other example is, let\u2019s say you have one pod running on worker node1 and your pod written some data now on local worker node1. But Due to some reasons your pod is rescheduled to run on worker node 2, how about your data written on worker node1? Your pod will be running on worker node 2 now, but your data won\u2019t be available here on worker node 2 since your data written by pod1 exists only on worker node1. So we must have shared volume that should be accessible across all worker nodes only when pods need it. In this case, persistent volume and persistent volume claim can be used at the kubernetes cluster level. But there is a traditional method to have shared volume across worker nodes at operating system level by mounting some volume through nfs, fc, iscsi on all worker nodes that can share the same volume. This example is discussed in the previous article. Before I explain you how to create persistent volume and persistent volume claim, Let me explain you what is actually happening in persistent volume and how it works? In a legacy infrastructure environment, when you need additional storage space to your server, you will reach out to the storage administrator for the space. So there would be a storage administrator who allocates some storage space from storage device to your server as you requested. Similarly, in kubernetes. Persistent volume is a resource type through which you can get your storage space allocated to your kubernetes cluster. Let's say you got some 10G persistent volume allocated to your kubernetes cluster. Obviously that should be through any one of the kubernetes volume types. Might be through ISCSI, FC, NFS, or any other cloud providers. From which you can claim some space you want for your pod using persistent volume claim. Let's say you want 5 GB for your pod. You can use persistent volume claim to request 5 GB space from your persistent volume. Now you persistent volume will allocates the space you requested using persistent volume when it is find suitable, now you can use that volume claim in your deployment. Let\u2019s see how to create Persistent Volume. Create a yaml file for persistent volume to get the storage space for our kubernetes cluster. Create a yaml file to claim the space using peristent volume claim as per our requirement. Define the persistent volume claim in your pod deployment file. Already I have a single pod running on worker node 1 with two containers. Sample deployment file is given below. It doesn\u2019t have any volume specification. Let's see how to use persistent volume and claim. kind: Deployment apiVersion: apps/v1 metadata: name: ebay-app spec: selector: matchLabels: environment: dev app: ebay replicas: 1 template: metadata: labels: environment: dev app: ebay spec: containers: name: container1-nginx image: nginx name: container2-tomcat image: tomcat I have an nfs server that acts as a storage and exported a volume named /nfsdata from 192.168.1.7. Traditional way is to mount the share in all worker nodes, instead we will be using this share through persistent volume. Right. So create a persistent volume yaml file. cat nfs_pv.yaml apiVersion: v1 kind: PersistentVolume metadata: name: ebay-pv spec: capacity: storage: 20Gi volumeMode: Filesystem accessModes: ReadWriteOnce persistentVolumeReclaimPolicy: Recycle storageClassName: ebaystorage mountOptions: nfsvers=4.1 nfs: path: /nfsdata server: 192.168.1.7 Persistent Volume supports three types of Reclaim Policy Retain Delete Recycle Persistent Volume supports three types of access modes ReadWriteOnce ReadOnlyMany ReadWriteMany Let\u2019s apply the changes and verify it. user1@kubernetes-master:~/codes/pv$ kubectl apply -f nfs_pv.yaml persistentvolume/ebay-pv created user1@kubernetes-master:~/codes/pv$ kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE ebay- pv 20Gi RWO Recycle Available ebaystora ge 24s Above output shows that pv \"ebay-pv\" is created as expected and it is available for claim. Let\u2019s create persistent volume claim: user1@kubernetes-master:~/codes/ebay$ cat pvc.yaml apiVersion: v1 kind: PersistentVolumeClaim metadata: name: myclaim spec: storageClassName: ebaystorage accessModes: ReadWriteOnce resources: requests: storage: 20G Claim can be given from kubernetes cluster only when it finds suitable any Storageclassname and accessmode are same as specified in this claim file, if any persistent volume doesn\u2019t have these storageclassname or accessmode, then persistent volume claim will not be processed. Let\u2019s apply this and verify it. user1@kubernetes-master:~/codes/ebay$ kubectl apply -f pvc.yaml persistentvolumeclaim/myclaim created user1@kubernetes-master:~/codes/ebay$ kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE myclaim Bound ebay-pv 20Gi RWO ebaystorage 15s So our claim is validated and allocated for us. Now we can use this claim to our pods. Edit your deployment file as below to define the volume specification. I will be using this volume only for my first container. user1@kubernetes-master:~/codes/ebay$ cat httpd-basic-deployment.yaml kind: Deployment apiVersion: apps/v1 metadata: name: ebay-app spec: selector: matchLabels: environment: dev app: ebay replicas: 1 template: metadata: labels: environment: dev app: ebay spec: volumes: name: myvolume persistentVolumeClaim: claimName: myclaim containers: name: container1-nginx image: nginx volumeMounts: name: myvolume mountPath: \"/tmp/persistent\" name: container2-tomcat image: tomcat Just apply the changes. user1@kubernetes-master:~/codes/ebay$ kubectl apply -f httpd-basic- deployment.yaml deployment.apps/ebay-app configured Use \"describe\" option to find the volume parameters and confirm the claim is successful. it should looks like this. user1@kubernetes-master:~/codes/ebay$ kubectl describe pods ebay-app ........trimmed some content...... Volumes: myvolume: Type: PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace) ClaimName: myclaim ReadOnly: false default-token-2tqkb: Type: Secret (a volume populated by a Secret) SecretName: default-token-2tqkb Optional: false ........trimmed some content...... That\u2019s it, we have successfully created persistent volume persistent volume claim. Now when your pod is rescheduled to other worker node, your data will be still available. Kubernetes Volumes What is Kubernetes Volumes? Kubernetes Volumes are used to store data that should be accessible across all your containers running in a pod based on the requirement. What are the types of Kubernetes Volumes? Kubernetes supports many kind of storage types, these are determined by how it is created and assigned to pods. Local Node Types - emptyDIR, hostpath, local File Sharing types - nfs Storage types - fc, iscsi Special Purpose Types - Secret, Git repo Cloud Provider types - Vsphere, Cinder, awsElasticBlockStore, azureDisk, gcepersistentDisk Distributed filesystem types - glusterfs, cephfs Special type - persistent volume, persistent volume claim Note: emptyDIR - It\u2019s a type of storage types that writes data only in memory till the pods running. So you data will be erased when the pod is deleted. So it\u2019s not a persistent kind of types. hostpath, local, fc and other types are persistent kind only, but volume won\u2019t be available across the nodes. It will be available only on local nodes. So we may need to setup something shared volume using traditional storage mount across all the nodes. Persistent volume type volumes can be accessible across all the nodes. How to use kubernetes volumes to pod and containers? Use an option \"Volumes\" along with name and types as below in a deployment file for the entire PODS and use the \"volumeMounts\" along with mountPath where the volume to be mounted for the container. We must use the volume name unique and exactly as specified in specification for the containers. If not you will end up with error. Example: spec: volumes: name: volume hostPath: path: /mnt/data containers: name: container1-nginx image: nginx volumeMounts: name: volume mountPath: \"/var/nginx-data\" name: container2-tomcat image: tomcat Above example tells that, volume name \"volume\" specified in \"spec\" section with Path \"/mnt/data\" will be used as a volume for this entire pod. It will be mounted only on container \"container1-nginx\" since it is claimed to be mounted on path \"/var/nginx-data\" using \"volumeMounts\" option. How to assign a single volume to specific container in a pod? In order to use a volume only to specific container running in a pod, we must use volume mounts option. So that particular container will use the volume specified in spec. kind: Deployment apiVersion: apps/v1 metadata: name: ebay-app spec: selector: matchLabels: environment: dev app: ebay replicas: 1 template: metadata: labels: environment: dev app: ebay spec: volumes: name: volume hostPath: path: /mnt/data containers: name: container1-nginx image: nginx volumeMounts: name: volume mountPath: \"/var/nginx-data\" name: container2-tomcat image: tomcat So we have claimed the volume name \"volume\" from specification and mapped to the container \"container1-nginx\" that would mount the volume under \"/var/nginx-data\", this volume will be only available to the first container \"container1-nginx\" not to the second container \"container2- tomcat\". This is how we can assign a single volume to specific container in a pod. How to share a same volume to all containers within a pod? In order to share a same volume to all containers running in a pod, we must use volume mounts option in all containers. kind: Deployment apiVersion: apps/v1 metadata: name: ebay-app spec: selector: matchLabels: environment: dev app: ebay replicas: 1 template: metadata: labels: environment: dev app: ebay spec: volumes: name: volume hostPath: path: /mnt/data containers: name: container1-nginx image: nginx volumeMounts: name: volume mountPath: \"/var/nginx-data\" name: container2-tomcat image: tomcat volumeMounts: name: volume mountPath: \"/var/tomcat-data\" This time, we have used volumeMount option for both containers with different path, as per the code definition, same volume \"volume\" will be mount on both containers in path \"/var/nginx- data\" on container1-nginx and \"/var/tomcat-data\" on container2-tomcat respectively. How to assign a dedicated volumes to each container in a pod? In order to assign a dedicated volumes to each containers running in a pod, we must use volumes and volumemounts option in all containers accordingly as per the example given below. kind: Deployment apiVersion: apps/v1 metadata: name: ebay-app spec: selector: matchLabels: environment: dev app: ebay replicas: 1 template: metadata: labels: environment: dev app: ebay spec: volumes: name: volume1 hostPath: path: /mnt/data1 name: volume2 hostPath: path: /mnt/data2 containers: name: container1-nginx image: nginx volumeMounts: name: volume1 mountPath: \"/var/nginx-data\" name: container2-tomcat image: tomcat volumeMounts: name: volume2 mountPath: \"/var/tomcat-data\" As per the above example, volume1 will be used by the first container \"container1-nginx\" and volume2 will be used by the second container \"container2-tomcat\". This is how we can assign dedicated volumes to each containers running in a pod. How to assign a shared volume across all pods running on different worker nodes? Why do we actually need this setup is, so far we have seen volumes that is used only on single pod running on one worker node. So your data won\u2019t be available when pod is rescheduled to other node since your hostpath you have used is local directory. If you want your data to be available for all worker nodes, we must have shared volumes concepts to overcome such situation. We can use a special type ie PersistentVolume and PersistentVolumeClaim or our traditional approach that mount a shared volume from storage and use that mounted path in the deployment file. You can checkout this video for the traditional approach and will explain you about persistentvolume and persistentvolume claim in the next article. To get all persistent volumes kubectl get persistentvolume To get all persistent volumes Claims kubectl get persistentvolumeclaims -o wide Kubernetes Security Role-Based Access Control (RBAC) In order to fully grasp the idea of RBAC, we must understand that three elements are involved: Subjects: The set of users and processes that want to access the Kubernetes API. Resources: The set of Kubernetes API Objects available in the cluster. Examples include Pods, Deployments, Services, Nodes, and PersistentVolumes, among others. Verbs: The set of operations that can be executed to the resources above. Different verbs are available (examples: get, watch, create, delete, etc.), but ultimately all of them are Create, Read, Update or Delete (CRUD) operations. With these three elements in mind, the key idea of RBAC is the following: We want to connect subjects, API resources, and operations. In other words, we want to specify, given a user , which operations can be executed over a set of resources . So, if we think about connecting these three types of entities, we can understand the different RBAC API Objects available in Kubernetes. Roles: Will connect API Resources and Verbs. These can be reused for different subjects. These are binded to one namespace (we cannot use wildcards to represent more than one, but we can deploy the same role object in different namespaces). If we want the role to be applied cluster-wide, the equivalent object is called ClusterRoles. RoleBinding: Will connect the remaining entity-subjects. Given a role, which already binds API Objects and verbs, we will establish which subjects can use it. For the cluster- level, non-namespaced equivalent, there are ClusterRoleBindings. In the example below, we are granting the user jsalmeron the ability to read, list and create pods inside the namespace test. This means that jsalmeron will be able to execute these commands: But not these: Example yaml files: Another interesting point would be the following: now that the user can create pods, can we limit how many? In order to do so, other objects, not directly related to the RBAC specification, allow configuring the amount of resources: ResourceQuota and LimitRanges . It is worth checking them out for configuring such a vital aspect of the cluster. Users and\u2026 ServiceAccounts? One topic that many Kubernetes users struggle with is the concept of subjects, but more specifically the difference between regular users and ServiceAccounts. In theory it looks simple: Users: These are global, and meant for humans or processes living outside the cluster. ServiceAccounts: These are namespaced and meant for intra-cluster processes running inside pods. Both have in common that they want to authenticate against the API in order to perform a set of operations over a set of resources (remember the previous section), and their domains seem to be clearly defined. They can also belong to what is known as groups, so a RoleBinding can bind more than one subject (but ServiceAccounts can only belong to the \u201csystem:serviceaccounts\u201d group). However, the key difference is a cause of several headaches: users do not have an associated Kubernetes API Object. That means that while this operation exists: This one doesn\u2019t: This has a vital consequence: if the cluster will not store any information about users, then, the administrator will need to manage identities outside the cluster. There are different ways to do so: TLS certificates, tokens, and OAuth2, among others. In addition, we would need to create kubectl contexts so we could access the cluster with these new credentials. In order to create the credential files, we could use the kubectl config commands (which do not require any access to the Kubernetes API, so they could be executed by any user). Watch the video above to see a complete example of user creation with TLS certificates. Possible operations over these resources are: create get delete list update edit watch exec Use case: Create user with limited namespace access In this example, we will create the following User Account: Username: employee Group: bitnami We will add the necessary RBAC policies so this user can fully manage deployments (i.e. use kubectl run command) only inside the office namespace. At the end, we will test the policies to make sure they work as expected. Step 1: Create the office namespace Execute the kubectl create command to create the namespace (as the admin user): kubectl create namespace office Step 2: Create the user credentials As previously mentioned, Kubernetes does not have API Objects for User Accounts. Of the available ways to manage authentication (see Kubernetes official documentation for a complete list), we will use OpenSSL certificates for their simplicity. The necessary steps are: Create a private key for your user. In this example, we will name the file employee.key : openssl genrsa -out employee.key 2048 Create a certificate sign request employee.csr using the private key you just created ( employee.key in this example). Make sure you specify your username and group in the - subj section (CN is for the username and O for the group). As previously mentioned, we will use employee as the name and bitnami as the group: openssl req -new -key employee.key -out employee.csr -subj \"/CN=employee/O=bitnami\" Locate your Kubernetes cluster certificate authority (CA). This will be responsible for approving the request and generating the necessary certificate to access the cluster API. Its location is normally /etc/kubernetes/pki/ . In the case of Minikube, it would be ~/.minikube/ . Check that the files ca.crt and ca.key exist in the location. Generate the final certificate employee.crt by approving the certificate sign request, employee.csr , you made earlier. Make sure you substitute the CA_LOCATION placeholder with the location of your cluster CA. In this example, the certificate will be valid for 500 days: openssl x509 -req -in employee.csr -CA CA_LOCATION/ca.crt -CAkey CA_LOCATION/ca.key -CAcreateserial -out employee.crt -days 500 Save both employee.crt and employee.key in a safe location (in this example we will use /home/employee/.certs/ ). Add a new context with the new credentials for your Kubernetes cluster. This example is for a Minikube cluster but it should be similar for others: kubectl config set-credentials employee --client- certificate=/home/employee/.certs/employee.crt --client- key=/home/employee/.certs/employee.key kubectl config set-context employee-context --cluster=minikube -- namespace=office --user=employee Now you should get an access denied error when using the kubectl CLI with this configuration file. This is expected as we have not defined any permitted operations for this user. kubectl --context=employee-context get pods Step 3: Create the role for managing deployments Create a role-deployment-manager.yaml file with the content below. In this yaml file we are creating the rule that allows a user to execute several operations on Deployments, Pods and ReplicaSets (necessary for creating a Deployment), which belong to the core (expressed by \"\" in the yaml file), apps , and extensions API Groups: kind: Role apiVersion: rbac.authorization.k8s.io/v1beta1 metadata: namespace: office name: deployment-manager rules: apiGroups: [\"\", \"extensions\", \"apps\"] resources: [\"deployments\", \"replicasets\", \"pods\"] verbs: [\"get\", \"list\", \"watch\", \"create\", \"update\", \"patch\", \"delete\"] # You can also use [\"*\"] Create the Role in the cluster using the kubectl create role command: kubectl create -f role-deployment-manager.yaml Step 4: Bind the role to the employee user Create a rolebinding-deployment-manager.yaml file with the content below. In this file, we are binding the deployment-manager Role to the User Account employee inside the office namespace: kind: RoleBinding apiVersion: rbac.authorization.k8s.io/v1beta1 metadata: name: deployment-manager-binding namespace: office subjects: kind: User name: employee apiGroup: \"\" roleRef: kind: Role name: deployment-manager apiGroup: \"\" Deploy the RoleBinding by running the kubectl create command: kubectl create -f rolebinding-deployment-manager.yaml Step 5: Test the RBAC rule Now you should be able to execute the following commands without any issues: kubectl --context=employee-context run --image bitnami/dokuwiki mydokuwiki kubectl --context=employee-context get pods If you run the same command with the --namespace=default argument, it will fail, as the employee user does not have access to this namespace. kubectl --context=employee-context get pods --namespace=default Now you have created a user with limited permissions in your cluster. Secrets Secret - At the application level, Kubernetes secrets can store sensitive information (such as passwords, SSH keys, API keys or tokens) per cluster (a virtual cluster if using namespaces, physical otherwise). Kubernetes Secret can be injected into a Pod container either as an environment variable or mounted as a file. Using Kubernetes Secrets allows us to abstract sensitive data and configuration from application deployment. Note that secrets are accessible from any pod in the same cluster. Network policies for access to pods can be defined in a deployment. A network policy specifies how pods are allowed to communicate with each other and with other network endpoints. Note that storing sensitive data in a Kubernetes Secret does not make it secure. By default, all data in Kubernetes Secrets is stored as a plaintext encoded with base64. There are multiple ways of creating secrets in Kubernetes. Creating from txt files. Creating from yaml file. Creating From Text File In order to create secrets from a text file such as user name and password, we first need to store them in a txt file and use the following command. $ kubectl create secret generic tomcat-passwd \u2013-from-file = ./username.txt \u2013 fromfile = ./.password.txt Creating From Yaml File apiVersion: v1 kind: Secret metadata: name: tomcat-pass type: Opaque data: password: username: Creating the Secret $ kubectl create \u2013f Secret.yaml secrets/tomcat-pass Using Secrets Once we have created the secrets, it can be consumed in a pod or the replication controller as \u2212 Environment Variable Volume As Environment Variable In order to use the secret as environment variable, we will use env under the spec section of pod yaml file. env: name: SECRET_USERNAME valueFrom: secretKeyRef: name: mysecret key: tomcat-pass As Volume spec: volumes: name: \"secretstest\" secret: secretName: tomcat-pass containers: image: tomcat:7.0 name: awebserver volumeMounts: mountPath: \"/tmp/mysec\" name: \"secretstest\" **Secret Configuration as Environment Variable ** apiVersion: v1 kind: ReplicationController metadata: name: appname spec: replicas: replica_count template: metadata: name: appname spec: nodeSelector: resource-group: containers: name: appname image: imagePullPolicy: Always ports: containerPort: 3000 env: -----------------------------> 1 name: ENV valueFrom: configMapKeyRef: name: appname key: tomcat-secrets In the above code, under the env definition, we are using secrets as environment variable in the replication controller. Secrets as Volume Mount apiVersion: v1 kind: pod metadata: name: appname spec: metadata: name: appname spec: volumes: name: \"secretstest\" secret: secretName: tomcat-pass containers: image: tomcat: 8.0 name: awebserver volumeMounts: mountPath: \"/tmp/mysec\" name: \"secretstest\" **Kubernetes Logging & Monitoring ** Monitoring is one of the key component for managing large clusters. For this, we have a number of tools. Monitoring with Prometheus It is a monitoring and alerting system. It was built at SoundCloud and was open sourced in 2012. It handles the multi-dimensional data very well. Sematext Docker Agent It is a modern Docker-aware metrics, events, and log collection agent. It runs as a tiny container on every Docker host and collects logs, metrics, and events for all cluster node and containers. It discovers all containers (one pod might contain multiple containers) including containers for Kubernetes core services, if the core services are deployed in Docker containers. After its deployment, all logs and metrics are immediately available out of the box. Kubernetes Log Kubernetes containers\u2019 logs are not much different from Docker container logs. However, Kubernetes users need to view logs for the deployed pods. Hence, it is very useful to have Kubernetes-specific information available for log search, such as \u2212 Kubernetes namespace Kubernetes pod name Kubernetes container name Docker image name Kubernetes UID **Resources ** https://kubernetes.io/docs/ https://wiki.aquasec.com/display/containers/Kubernetes+101 https://www.ibm.com/cloud/architecture/content/course/kubernetes-101 https://kubernetesbyexample.com/ https://www.tutorialspoint.com/kubernetes/index.htm https://unofficial-kubernetes.readthedocs.io/en/latest/ https://www.katacoda.com/courses/kubernetes https://access.redhat.com/documentation/en- us/red_hat_enterprise_linux_atomic_host/7/html/getting_started_with_kubernetes/index 71*","title":"K8-Docs"},{"location":"k8-docs/#kubectl-api-resources","text":"Hints: vsCode is an excellent tool to make the YAML file. You can use Helm or Bitnami as chart and ready apps You can use container platform and service catalog. Kubectl Autocomplete","title":"kubectl api-resources"},{"location":"k8-docs/#source-kubectl-completion-bash-setup-autocomplete-in-bash-into-the-current-shell-bash-completion-package-should-be-installed-first","text":"","title":"source &lt;(kubectl completion bash) # setup autocomplete in bash into the current shell, bash-completion package should be installed first."},{"location":"k8-docs/#echo-source-kubectl-completion-bash-bashrc-add-autocomplete-permanently-to-your-bash-shell","text":"Install kubernetes Step 1 - Kubernetes Installation The three-node cluster that we will be forming in this example will consist of a Master node and a Two Slave nodes, therefore, follow the steps described below to install Kubernetes on the CentOS nodes. Kubernetes will be installed as a container so we must install Docker engine All nodes need to have Kubernetes installed on them. We will prepare all servers for Kubernetes installation by changing the existing configuration on servers, and also installing some packages, including docker-ce and kubernetes itself. Configure Hosts File","title":"echo \"source &lt;(kubectl completion bash)\" &gt;&gt; ~/.bashrc # add autocomplete permanently to your bash shell."},{"location":"k8-docs/#vim-etchosts","text":"Add all servers IPs master and workers to the hosts file below. 192.168.179.133 k8s-master 192.168.179.131 node01 192.168.179.132 node02 Save and exit. -Configure Firewall The nodes, containers, and pods need to be able to communicate across the cluster to perform their functions. Firewalld is enabled in CentOS by default on the front-end. Add the following ports by entering the listed commands. On the Master Node enter:","title":"vim /etc/hosts"},{"location":"k8-docs/#firewall-cmd-permanent-add-port6443tcp","text":"firewall-cmd --permanent --add-port=2379-2380/tcp firewall-cmd --permanent --add-port=10250/tcp firewall-cmd --permanent --add-port=10251/tcp firewall-cmd --permanent --add-port=10252/tcp firewall-cmd --permanent --add-port=10255/tcp firewall-cmd --reload Enter the following commands on each worker node:","title":"firewall-cmd --permanent --add-port=6443/tcp"},{"location":"k8-docs/#firewall-cmd-permanent-add-port10251tcp","text":"firewall-cmd --permanent --add-port=10252/tcp firewall-cmd \u2013reload To list all firewall rules","title":"firewall-cmd --permanent --add-port=10251/tcp"},{"location":"k8-docs/#firewall-cmd-list-all","text":"Update Iptables Settings Set the net.bridge.bridge-nf-call-iptables to \u20181\u2019 in your sysctl config file. This ensures that packets are properly processed by IP tables during filtering and port forwarding.","title":"firewall-cmd --list-all"},{"location":"k8-docs/#cat-etcsysctldk8sconf-netbridgebridge-nf-call-ip6tables-1-netbridgebridge-nf-call-iptables-1-eof","text":"sysctl --system Enable br_netfilter Kernel Module The br_netfilter module is required for kubernetes installation. Enable this kernel module so that the packets traversing the bridge are processed by iptables for filtering and for port forwarding, and the kubernetes pods across the cluster can communicate with each other. Run the command below to enable the br_netfilter kernel module.","title":"cat &lt; /etc/sysctl.d/k8s.conf net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 EOF"},{"location":"k8-docs/#modprobe-br_netfilter","text":"","title":"modprobe br_netfilter"},{"location":"k8-docs/#echo-1-procsysnetbridgebridge-nf-call-iptables","text":"-Disable SELinux","title":"echo '1' &gt; /proc/sys/net/bridge/bridge-nf-call-iptables"},{"location":"k8-docs/#sudo-setenforce-0","text":"","title":"sudo setenforce 0"},{"location":"k8-docs/#sudo-sed-i-sselinuxenforcingselinuxpermissive-etcselinuxconfig","text":"Disable SWAP If swap is not disabled, kubelet service will not start on the masters and nodes, for Platform9 Managed Kubernetes version 3.3 and above.","title":"sudo sed -i \u2018s/^SELINUX=enforcing$/SELINUX=permissive/\u2019 /etc/selinux/config"},{"location":"k8-docs/#swapoff-a","text":"And then edit the '/etc/fstab' file. Comment the swap line UUID as below. **Install Docker on the nodes ** Install the package dependencies for docker-ce.","title":"swapoff \u2013a"},{"location":"k8-docs/#yum-install-y-yum-utils-device-mapper-persistent-data","text":"Add the docker repository to the system and install docker-ce using the yum command.","title":"yum install -y yum-utils device-mapper-persistent-data"},{"location":"k8-docs/#yum-config-manager-add-repo-httpsdownloaddockercomlinuxcentosdocker-cerepo","text":"","title":"yum-config-manager --add-repo https://download.docker.com/linux/centos/docker- ce.repo"},{"location":"k8-docs/#yum-install-y-docker-ce","text":"Enable Docker on the nodes Enable and start the Docker utility on both the nodes by running the following command on each:","title":"yum install -y docker-ce"},{"location":"k8-docs/#systemctl-enable-docker","text":"","title":"systemctl enable docker"},{"location":"k8-docs/#systemctl-start-docker","text":"Add the Kubernetes signing key on the nodes Add the kubernetes repository and key to the centos 7 system by running the following command.","title":"systemctl start docker"},{"location":"k8-docs/#cat-etcyumreposdkubernetesrepo","text":"[kubernetes] name=Kubernetes baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64 enabled=1 gpgcheck=1 repo_gpgcheck=1 gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg EOF Install Kubeadm Now install the kubernetes packages kubeadm, kubelet, and kubectl using the yum command below.","title":"cat &lt; /etc/yum.repos.d/kubernetes.repo"},{"location":"k8-docs/#yum-install-y-kubelet-kubeadm-kubectl","text":"You can check the version number of Kubeadm and also verify the installation through the following command:","title":"yum install -y kubelet kubeadm kubectl"},{"location":"k8-docs/#kubeadm-version","text":"After the installation is complete, restart all those servers. Log in again to the server and start the services, docker and kubelet.","title":"kubeadm version"},{"location":"k8-docs/#systemctl-start-docker-systemctl-enable-docker","text":"","title":"systemctl start docker &amp;&amp; systemctl enable docker"},{"location":"k8-docs/#systemctl-start-kubelet-systemctl-enable-kubelet","text":"Change the cgroup-driver We need to make sure the docker-ce and kubernetes are using same 'cgroup'. Check docker cgroup using the docker info command.","title":"systemctl start kubelet &amp;&amp; systemctl enable kubelet"},{"location":"k8-docs/#docker-info-grep-i-cgroup","text":"And you see the docker is using 'cgroupfs' as a cgroup-driver. Now run the command below to change the kuberetes cgroup-driver to 'cgroupfs'.","title":"docker info | grep -i cgroup"},{"location":"k8-docs/#sed-i-scgroup-driversystemdcgroup-drivercgroupfsg-etcsystemdsystemkubeletserviced10-kubeadmconf","text":"Step 2 - Kubernetes Cluster Initialization In this step, we will initialize the kubernetes master cluster configuration. Move the shell to the master server 'master' and run the command below to set up the kubernetes master.","title":"sed -i 's/cgroup-driver=systemd/cgroup-driver=cgroupfs/g' /etc/systemd/system/kubelet.service.d/10-kubeadm.conf"},{"location":"k8-docs/#kubeadm-init-apiserver-advertise-address1001510-pod-network-cidr102440016","text":"--apiserver-advertise-address = determines which IP address Kubernetes should advertise its API server on. --pod-network-cidr = specify the range of IP addresses for the pod network. We're using the 'flannel' virtual network. If you want to use another pod network such as weave-net or calico, change the range IP address. When the Kubernetes initialization is complete, you will get the result as below. Copy the ' kubeadm join ... ... ... ' command to your text editor. The command will be used to register new nodes to the kubernetes cluster. Now in order to use Kubernetes, we need to run some commands as on the result. Create new '.kube' configuration directory and copy the configuration 'admin.conf'.","title":"kubeadm init --apiserver-advertise-address=10.0.15.10 --pod-network- cidr=10.244.0.0/16"},{"location":"k8-docs/#mkdir-p-homekube","text":"sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config Next, deploy the flannel network to the kubernetes cluster using the kubectl command.","title":"mkdir -p $HOME/.kube"},{"location":"k8-docs/#kubectl-apply-f-httpsrawgithubusercontentcomcoreosflannelmasterdocumentationkube-flannelyml","text":"The flannel network has been deployed to the Kubernetes cluster. Wait for a minute and then check kubernetes node and pods using commands below.","title":"kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube- flannel.yml"},{"location":"k8-docs/#kubectl-get-nodes","text":"","title":"kubectl get nodes"},{"location":"k8-docs/#kubectl-get-pods-all-namespaces","text":"And you will get the \u2018master' node is running as a 'master' cluster with status 'ready', and you will get all pods that are needed for the cluster, including the 'kube-flannel-ds' for network pod configuration. Make sure all kube-system pods status is 'running'. Kubernetes cluster master initialization and configuration has been completed. Step 3 - Adding node1 and node2 to the Cluster In this step, we will add node1 and node2 to join the 'master' cluster. Connect to the node1 server and run the kubeadm join command as we copied on the top.","title":"kubectl get pods --all-namespaces"},{"location":"k8-docs/#kubeadm-join-1921681791336443-token-l12jxw3c960ner9320dpg8-discovery-token-ca-cert-hash-sha256af54557ca10c0702f6e29bf4d7d96a76eea38e8c85a1bc5cfd6d56ee9d9a1dd7","text":"Connect to the node2 server and run the kubeadm join command as we copied on the top. Wait for some minutes and back to the \u2018master' master cluster server check the nodes and pods using the following command.","title":"kubeadm join 192.168.179.133:6443 --token l12jxw.3c960ner9320dpg8     --discovery- token-ca-cert-hash sha256:af54557ca10c0702f6e29bf4d7d96a76eea38e8c85a1bc5cfd6d56ee9d9a1dd7"},{"location":"k8-docs/#kubectl-get-nodes_1","text":"","title":"kubectl get nodes"},{"location":"k8-docs/#kubectl-get-pods-all-namespaces_1","text":"Node1 and Node2 have been added to the kubernetes cluster. Step 4 - Create First Pod In this step, we will do a test by deploying the Nginx pod to the kubernetes cluster. Configure the yaml file of deployment and if want to publish it from external network must to configure service yaml file Pod yaml apiVersion: v1 kind: Pod metadata: name: website labels: name: web spec: containers: name: web-server image: nginx resources: limits: memory: \"128Mi\" cpu: \"500m\" ports: containerPort: 80 Service yaml apiVersion: v1 kind: Service metadata: name: Publish-website spec: selector: name: web type: NodePort ports: name: http port: 80 targetPort: 80 nodePort: 30003 protocol: TCP Above example selector name is the label name on the pod deployment yaml file Publish containers from nodeport type use ports from 30000 to 32000. Create new deployment named 'nginx' using the kubectl command.","title":"kubectl get pods --all-namespaces"},{"location":"k8-docs/#kubectl-create-f-websiteyaml","text":"To see details of the 'nginx' deployment sepcification, run the following command.","title":"kubectl create -f website.yaml"},{"location":"k8-docs/#kubectl-describe-deployment-nginx","text":"Next, we will expose the nginx pod accessible via the internet. And we need to create new service NodePort for this.","title":"kubectl describe deployment nginx"},{"location":"k8-docs/#kubectl-create-f-serviceyaml","text":"If I want to change anything in yaml configuration and reapply it use command.","title":"kubectl create -f service.yaml"},{"location":"k8-docs/#kubectl-apply-f-serviceyaml","text":"Make sure there is no error. Now check the nginx service nodeport and IP using the kubectl command below.","title":"kubectl apply -f service.yaml"},{"location":"k8-docs/#kubectl-get-pods","text":"","title":"kubectl get pods"},{"location":"k8-docs/#kubectl-get-svc","text":"","title":"kubectl get svc"},{"location":"k8-docs/#kubectl-describe-svc","text":"Now you will get the nginx pod is now running under cluster IP address \u201810.109.154.222\u2019 port 80, and the node main IP address '192.168.179.131' on port '30003'. The Nginx Pod has now been deployed under the Kubernetes cluster and it's accessible via the internet, now access from the web browser. http://192.168.179.131:30003/ Kubernetes Namespace **What is a Namespace? ** You can think of a Namespace as a virtual cluster inside your Kubernetes cluster. You can have multiple namespaces inside a single Kubernetes cluster, and they are all logically isolated from each other. They can help you and your teams with organization, security, and even performance!** In most Kubernetes distributions, the cluster comes out of the box with a Namespace called \u201cdefault.\u201d In fact, there are actually three namespaces that Kubernetes ships with: default, kube- system (used for Kubernetes components), and kube-public (used for public resources). Kube- public isn\u2019t really used for much right now, and it\u2019s usually a good idea to leave kube-system alone Functionality of Namespace Following are some of the important functionalities of a Namespace in Kubernetes \u2212 Namespaces help pod-to-pod communication using the same namespace. Namespaces are virtual clusters that can sit on top of the same physical cluster. They provide logical separation between the teams and their environments. Create a Namespace The following command is used to create a namespace. apiVersion: v1 kind: Namespace metadata name: MicroFinance Control the Namespace The following command is used to control the namespace. kubectl create \u2013f namespace.yml ---------> 1 kubectl get namespace -----------------> 2 kubectl get namespace ------->3 kubectl describe namespace ---->4 kubectl delete namespace In the above code, We are using the command to create a namespace. This will list all the available namespace. This will get a particular namespace whose name is specified in the command. This will describe the complete details about the service. This will delete a particular namespace present in the cluster. This is a sample resource quota limit for namespace apiVersion: v1 kind: ResourceQouta metadata: name: compute-qouta namespace: prod spec: hard: pods: \"10\" requests.cpu: \"4\" requests.memory: 5Gi limits.cpu: \"10\" limits.memory: 10Gi This is the output after apply limits on namespace","title":"kubectl describe svc"},{"location":"k8-docs/#kubectl-describe-namespace-prod","text":"Using Namespace in Service - Example Following is an example of a sample file for using namespace in service. apiVersion: v1 kind: Service metadata: name: Micro namespace: MicroFinance labels: component: Micro spec: type: LoadBalancer selector: component: Micro ports: name: http port: 9200 protocol: TCP name: transport port: 9300 protocol: TCP In the above code, we are using the same namespace under service metadata with the name of MicroFinance. Kubernetes Deployments Deployments are upgraded and higher version of replication controller. They manage the deployment of replica sets which is also an upgraded version of the replication controller. They have the capability to update the replica set and are also capable of rolling back to the previous version. They provide many updated features of matchLabels and selectors . We have got a new controller in the Kubernetes master called the deployment controller which makes it happen. It has the capability to change the deployment midway. Changing the Deployment Updating \u2212 The user can update the ongoing deployment before it is completed. In this, the existing deployment will be settled and new deployment will be created. Deleting \u2212 The user can pause/cancel the deployment by deleting it before it is completed. Recreating the same deployment will resume it. Rollback \u2212 We can roll back the deployment or the deployment in progress. The user can create or update the deployment by using DeploymentSpec.PodTemplateSpec = oldRC.PodTemplateSpec. Deployment Strategies Deployment strategies help in defining how the new RC should replace the existing RC. Recreate \u2212 This feature will kill all the existing RC and then bring up the new ones. This results in quick deployment however it will result in downtime when the old pods are down and the new pods have not come up. Rolling Update \u2212 This feature gradually brings down the old RC and brings up the new one. This results in slow deployment, however there is no deployment. At all times, few old pods and few new pods are available in this process. The configuration file of Deployment looks like this. apiVersion: extensions/v1beta1 --------------------->1 kind: Deployment --------------------------> 2 metadata: name: Tomcat-ReplicaSet spec: replicas: 3 template: metadata: lables: app: Tomcat-ReplicaSet tier: Backend spec: containers: name: Tomcatimage: tomcat: 8.0 ports: containerPort: 7474 In the above code, the only thing which is different from the replica set is we have defined the kind as deployment. **Create Deployment **","title":"kubectl describe namespace prod"},{"location":"k8-docs/#kubectl-create-f-deploymentyaml-record","text":"-\u2013record to see the deployments by command # kubectl rollout status deployment/Deployment Deployment \"Deployment\" created successfully. Fetch the Deployment","title":"kubectl create \u2013f Deployment.yaml \u2013record"},{"location":"k8-docs/#kubectl-get-deployments","text":"","title":"kubectl get deployments"},{"location":"k8-docs/#kubectl-get-deploy-o-wide","text":"NAME DESIRED CURRENT UP-TO-DATE AVILABLE AGE Deployment 3 3 3 3 20s Check the Status of Deployment","title":"kubectl get deploy -o wide"},{"location":"k8-docs/#kubectl-rollout-status-deploymentdeployment","text":"Updating the Deployment","title":"kubectl rollout status deployment/Deployment"},{"location":"k8-docs/#kubectl-apply-f-deploymentyaml-after-update-the-new-version-inside-the-yaml-file","text":"","title":"kubectl apply \u2013f Deployment.yaml (after update the new version inside the yaml file)"},{"location":"k8-docs/#kubectl-set-image-deploymentdeployment-tomcattomcat60","text":"Rolling Back to Previous Deployment","title":"kubectl set image deployment/Deployment tomcat=tomcat:6.0"},{"location":"k8-docs/#kubectl-rollout-undo-deploymentdeployment-to-revision2","text":"Check rollout history","title":"kubectl rollout undo deployment/Deployment \u2013to-revision=2"},{"location":"k8-docs/#kubectl-rollout-history-deploymentdeployment","text":"Get ReplicaSet","title":"kubectl rollout history deployment/Deployment"},{"location":"k8-docs/#kubectl-get-replicaset","text":"[root@master ~]# kubectl get replicaset NAME DESIRED CURRENT READY AGE Tomcat-ReplicaSet 3 3 3 2d14h Delete ReplicaSet (it will also delete all underlying Pods)","title":"kubectl get replicaset"},{"location":"k8-docs/#kubectl-delete-replicaset-replicaset-name","text":"[root@master ~]# delete replicaset Tomcat-ReplicaSet Scaling the Deployment","title":"kubectl delete ReplicaSet &lt; ReplicaSet name&gt;"},{"location":"k8-docs/#kubectl-scale-replicas6-f-deploymentyaml","text":"To deploy from local image include imagePullPolicy: Never in the deployment yaml file spec: containers: name: nginx-web image: nginx resources: limits: memory: \"100Mi\" cpu: \"100m\" imagePullPolicy: Never Assign Pods to Nodes 1- Assign label to your nodes List the nodes in your cluster, along with their labels:","title":"kubectl scale --replicas=6  \u2013f Deployment.yaml"},{"location":"k8-docs/#kubectl-get-nodes-show-labels","text":"Chose one of your nodes, and add a label to it:","title":"kubectl get nodes --show-labels"},{"location":"k8-docs/#kubectl-label-nodes-centos-typemaster","text":"Verify that your chosen node has a type=Master label: 2- In the YAML file add selector of nodeSelector with the label spec: containers: name: nginx image: nginx nodeSelector: type: Master Apply your deployment","title":"kubectl label nodes centos type=Master"},{"location":"k8-docs/#kubectl-apply-f-deploymentyaml","text":"To get into Pod Shell","title":"kubectl apply -f Deployment.yaml"},{"location":"k8-docs/#kubectl-exec-stdin-tty-binbash","text":"[root@master ~]# kubectl exec --stdin --tty pod/nginx-deployment-597fbc7d6f- jhwf2 -- /bin/bash root@nginx-deployment-597fbc7d6f-jhwf2:/# ls Kubernetes Networking Networking in any architecture or technology is very essential to understand if you want to deploy the applications over the network and also understanding how the network works that will help you to troubleshoot if you have any network issue. So we must know exactly what is happening in the network and how all the connections are establishing for the communication over the network. This is a basic kubernetes architecture where you have some worker nodes, and few pods are running on it with couple of containers. When you have these many components, how all these components are establishing its connection to make the application accessible over the network? The answer is that happens only through kubernetes networking. Types of Kubernetes Networking Container to Container Communication: When you have one or more containers within a pod that shares the same host networking. So pods will get its own IP address, all container shares same ip address but it works on different port. Communication between containers happens within the pod itself on different port. So all containers will be able to communicate each other by default. Pod to Pod communication: As said earlier, each pods will get its own ip address. There are sub types within Pod to Pod communication, that is. Intra-node Pod Network - Communication of pods running on a single node. Inter-node Pod Network - Communication of pods running in different nodes. On the first case, each pod running on single worker node will have the communication by default, because all ip address of pods will be different and assigned from your local network. Since it shares the same host. On the second case, when you have pod running on multiple worker nodes, communication between these pods happens through network plugin that will create some route tables. It forwards the traffic from any pod to any destination pods. Pod to Service Communication: Service is kubernetes resource type that expose our application to outside the cluster. Through which pod can send the traffic to services. 4 External to Service Communication: In order to access our application from outside the cluster, external traffic should be allowed to reach the server within the cluster... This can be achieved using these different types. ClusterIP NodePort LoadBalancer Extername Each types has its own function and purpose. Cluster IP - It is the default kubernetes service used for internal communication within the cluster. NodePort - It will open a ports on each nodes and traffic will be forwarded to the service through random port. And I can access the service (Pod) with the node IP + Defined port. LoadBalancer - It is a type that forwards all external traffic to a service through this type. And I can access the service (Pod) with the node name only. External Name - it is a type used to access a service internally that is hosted outside cluster through DNS CName or A record... LoadBalancer service is not in kubernetes by default, to use it we must create ingress network. Ingress Policy Ingress exposes HTTP and HTTPS routes from outside the cluster to services within the cluster. Traffic routing is controlled by rules defined on the Ingress resource. internet | [ Ingress ] --|-----|-- [ Services ] An Ingress may be configured to give Services externally-reachable URLs, load balance traffic, terminate SSL / TLS, and offer name based virtual hosting. An Ingress controller is responsible for fulfilling the Ingress, usually with a load balancer, though it may also configure your edge router or additional frontends to help handle the traffic. An Ingress does not expose arbitrary ports or protocols. Exposing services other than HTTP and HTTPS to the internet typically uses a service of type Service.Type=NodePort or Service.Type=LoadBalancer . Prerequisites You must have an ingress controller to satisfy an Ingress. Only creating an Ingress resource has no effect. You may need to deploy an Ingress controller such as ingress-nginx. The Ingress Resource A minimal Ingress resource example: apiVersion: extensions/v1beta1 kind: Ingress metadata: name: test-ingress namespace: critical-space annotations: nginx.ingress.kubernetes.io/rewrite-target: / spec: rules: http: paths: path: /website backend: serviceName: website-service servicePort: 80 Then I make deployment with type LoadBalancer apiVersion: v1 kind: Service metadata: name: mywebsite spec: ports: name: http port: 80 protocol: TCP targetPort: 80 type: LoadBalancer selector: Ingress and egress The bulk of securing network traffic typically revolves around defining egress and ingress rules. From the point of view of a Kubernetes pod, ingress is incoming traffic to the pod, and egress is outgoing traffic from the pod. In Kubernetes network policy, you create ingress and egress \u201callow\u201d rules independently (egress, ingress, or both). Kubernetes NodePort vs LoadBalancer vs Ingress? When should I use what? They are all different ways to get external traffic into your cluster ClusterIP A ClusterIP service is the default Kubernetes service. It gives you a service inside your cluster that other apps inside your cluster can access. There is no external access. The YAML for a ClusterIP service looks like this: apiVersion: v1 kind: Service metadata: name: my-internal-service spec: selector: app: my-app type: ClusterIP ports: name: http port: 80 targetPort: 80 protocol: TCP If you can\u2019t access a ClusterIP service from the internet, Turns out you can access it using the Kubernetes proxy! Start the Kubernetes Proxy: kubectl proxy --port=8080 Now, you can navigate through the Kubernetes API to access this service using this scheme: http://localhost:8080/api/v1/proxy/namespaces/ /services/ : / So to access the service we defined above, you could use the following address: http://localhost:8080/api/v1/proxy/namespaces/default/services/my-internal-service:http/ When would you use this? There are a few scenarios where you would use the Kubernetes proxy to access your services. Debugging your services, or connecting to them directly from your laptop for some reason Allowing internal traffic, displaying internal dashboards, etc. Because this method requires you to run kubectl as an authenticated user, you should NOT use this to expose your service to the internet or use it for production services. NodePort A NodePort service is the most primitive way to get external traffic directly to your service. NodePort, as the name implies, opens a specific port on all the Nodes (the VMs), and any traffic that is sent to this port is forwarded to the service. The YAML for a NodePort service looks like this: apiVersion: v1 kind: Service metadata: name: my-nodeport-service spec: selector: app: my-app type: NodePort ports: name: http port: 80 targetPort: 80 nodePort: 30036 protocol: TCP NodePort service has two differences from a normal \u201cClusterIP\u201d service. First, the type is \u201cNodePort.\u201d There is also an additional port called the NodePort that specifies which port to open on the nodes. If you don\u2019t specify this port, it will pick a random port. Most of the time you should let Kubernetes choose the port; there are many caveats to what ports are available for you to use. When would you use this? There are many downsides to this method: You can only have one service per port You can only use ports 30000\u201332767 If your Node/VM IP address change, you need to deal with that For these reasons, I don\u2019t recommend using this method in production to directly expose your service. If you are running a service that doesn\u2019t have to be always available, or you are very cost sensitive, this method will work for you. A good example of such an application is a demo app or something temporary. **LoadBalancer ** A LoadBalancer service is the standard way to expose a service to the internet. This will spin up a Network Load Balancer that will give you a single IP address that will forward all traffic to your service. When would you use this? If you want to directly expose a service, this is the default method. All traffic on the port you specify will be forwarded to the service. There is no filtering, no routing, etc. This means you can send almost any kind of traffic to it, like HTTP, TCP, UDP, Websockets, gRPC, or whatever. The big downside is that each service you expose with a LoadBalancer will get its own IP address, and you have to pay for a LoadBalancer per exposed service, which can get expensive! Ingress Unlike all the above examples, Ingress is actually NOT a type of service. Instead, it sits in front of multiple services and act as a \u201csmart router\u201d or entry point into your cluster. You can do a lot of different things with an Ingress, and there are many types of Ingress controllers that have different capabilities. The YAML for an Ingress object with a L7 HTTP Load Balancer might look like this: apiVersion: extensions/v1beta1 kind: Ingress metadata: name: my-ingress spec: backend: serviceName: other servicePort: 8080 rules: host: foo.mydomain.com http: paths: backend: serviceName: foo servicePort: 8080 host: mydomain.com http: paths: path: /bar/* backend: serviceName: bar servicePort: 8080 When would you use this? Ingress is probably the most powerful way to expose your services, but can also be the most complicated. There are many types of Ingress controllers, from the Google Cloud Load Balancer, Nginx, Contour, Istio, and more. There are also plugins for Ingress controllers, like the cert- manager, that can automatically provision SSL certificates for your services. Ingress is the most useful if you want to expose multiple services under the same IP address, and these services all use the same L7 protocol (typically HTTP). You only pay for one load balancer if you are using the native GCP integration, and because Ingress is \u201csmart\u201d you can get a lot of features out of the box (like SSL, Auth, Routing, etc). Kubernetes Storage Persistent volume claim (PVC) A Persistent Volume Claim (PVC) is a claim request for some storage space by users. Persistent volume (PV) A Persistent Volume (PV) is a piece of storage in the cluster that has been provisioned by an administrator or dynamically provisioned using Storage Classes. In simple words, Persistent Volume is a solution to store data of our containers permanently even after the pods got deleted. How it is different from other kubernetes volume types? Let\u2019s say you have multiple pod running on different nodes and you used hostpath volume type. Your data written by pod 1 running on worker node 1 will be resides only on worker node 1 and that cannot be access by pod 2 running on worker node 2. Similarly pod 1 cannot access data written by pod 2 on worker node 2. Right? Since hostpath is a type that writes data only on your local node directory. It\u2019s not kind of a shared volume. Other example is, let\u2019s say you have one pod running on worker node1 and your pod written some data now on local worker node1. But Due to some reasons your pod is rescheduled to run on worker node 2, how about your data written on worker node1? Your pod will be running on worker node 2 now, but your data won\u2019t be available here on worker node 2 since your data written by pod1 exists only on worker node1. So we must have shared volume that should be accessible across all worker nodes only when pods need it. In this case, persistent volume and persistent volume claim can be used at the kubernetes cluster level. But there is a traditional method to have shared volume across worker nodes at operating system level by mounting some volume through nfs, fc, iscsi on all worker nodes that can share the same volume. This example is discussed in the previous article. Before I explain you how to create persistent volume and persistent volume claim, Let me explain you what is actually happening in persistent volume and how it works? In a legacy infrastructure environment, when you need additional storage space to your server, you will reach out to the storage administrator for the space. So there would be a storage administrator who allocates some storage space from storage device to your server as you requested. Similarly, in kubernetes. Persistent volume is a resource type through which you can get your storage space allocated to your kubernetes cluster. Let's say you got some 10G persistent volume allocated to your kubernetes cluster. Obviously that should be through any one of the kubernetes volume types. Might be through ISCSI, FC, NFS, or any other cloud providers. From which you can claim some space you want for your pod using persistent volume claim. Let's say you want 5 GB for your pod. You can use persistent volume claim to request 5 GB space from your persistent volume. Now you persistent volume will allocates the space you requested using persistent volume when it is find suitable, now you can use that volume claim in your deployment. Let\u2019s see how to create Persistent Volume. Create a yaml file for persistent volume to get the storage space for our kubernetes cluster. Create a yaml file to claim the space using peristent volume claim as per our requirement. Define the persistent volume claim in your pod deployment file. Already I have a single pod running on worker node 1 with two containers. Sample deployment file is given below. It doesn\u2019t have any volume specification. Let's see how to use persistent volume and claim. kind: Deployment apiVersion: apps/v1 metadata: name: ebay-app spec: selector: matchLabels: environment: dev app: ebay replicas: 1 template: metadata: labels: environment: dev app: ebay spec: containers: name: container1-nginx image: nginx name: container2-tomcat image: tomcat I have an nfs server that acts as a storage and exported a volume named /nfsdata from 192.168.1.7. Traditional way is to mount the share in all worker nodes, instead we will be using this share through persistent volume. Right. So create a persistent volume yaml file.","title":"kubectl exec --stdin --tty  -- /bin/bash"},{"location":"k8-docs/#cat-nfs_pvyaml","text":"apiVersion: v1 kind: PersistentVolume metadata: name: ebay-pv spec: capacity: storage: 20Gi volumeMode: Filesystem accessModes: ReadWriteOnce persistentVolumeReclaimPolicy: Recycle storageClassName: ebaystorage mountOptions: nfsvers=4.1 nfs: path: /nfsdata server: 192.168.1.7 Persistent Volume supports three types of Reclaim Policy Retain Delete Recycle Persistent Volume supports three types of access modes ReadWriteOnce ReadOnlyMany ReadWriteMany Let\u2019s apply the changes and verify it. user1@kubernetes-master:~/codes/pv$ kubectl apply -f nfs_pv.yaml persistentvolume/ebay-pv created user1@kubernetes-master:~/codes/pv$ kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE ebay- pv 20Gi RWO Recycle Available ebaystora ge 24s Above output shows that pv \"ebay-pv\" is created as expected and it is available for claim. Let\u2019s create persistent volume claim: user1@kubernetes-master:~/codes/ebay$ cat pvc.yaml apiVersion: v1 kind: PersistentVolumeClaim metadata: name: myclaim spec: storageClassName: ebaystorage accessModes: ReadWriteOnce resources: requests: storage: 20G Claim can be given from kubernetes cluster only when it finds suitable any Storageclassname and accessmode are same as specified in this claim file, if any persistent volume doesn\u2019t have these storageclassname or accessmode, then persistent volume claim will not be processed. Let\u2019s apply this and verify it. user1@kubernetes-master:~/codes/ebay$ kubectl apply -f pvc.yaml persistentvolumeclaim/myclaim created user1@kubernetes-master:~/codes/ebay$ kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE myclaim Bound ebay-pv 20Gi RWO ebaystorage 15s So our claim is validated and allocated for us. Now we can use this claim to our pods. Edit your deployment file as below to define the volume specification. I will be using this volume only for my first container. user1@kubernetes-master:~/codes/ebay$ cat httpd-basic-deployment.yaml kind: Deployment apiVersion: apps/v1 metadata: name: ebay-app spec: selector: matchLabels: environment: dev app: ebay replicas: 1 template: metadata: labels: environment: dev app: ebay spec: volumes: name: myvolume persistentVolumeClaim: claimName: myclaim containers: name: container1-nginx image: nginx volumeMounts: name: myvolume mountPath: \"/tmp/persistent\" name: container2-tomcat image: tomcat Just apply the changes. user1@kubernetes-master:~/codes/ebay$ kubectl apply -f httpd-basic- deployment.yaml deployment.apps/ebay-app configured Use \"describe\" option to find the volume parameters and confirm the claim is successful. it should looks like this. user1@kubernetes-master:~/codes/ebay$ kubectl describe pods ebay-app ........trimmed some content...... Volumes: myvolume: Type: PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace) ClaimName: myclaim ReadOnly: false default-token-2tqkb: Type: Secret (a volume populated by a Secret) SecretName: default-token-2tqkb Optional: false ........trimmed some content...... That\u2019s it, we have successfully created persistent volume persistent volume claim. Now when your pod is rescheduled to other worker node, your data will be still available. Kubernetes Volumes What is Kubernetes Volumes? Kubernetes Volumes are used to store data that should be accessible across all your containers running in a pod based on the requirement. What are the types of Kubernetes Volumes? Kubernetes supports many kind of storage types, these are determined by how it is created and assigned to pods. Local Node Types - emptyDIR, hostpath, local File Sharing types - nfs Storage types - fc, iscsi Special Purpose Types - Secret, Git repo Cloud Provider types - Vsphere, Cinder, awsElasticBlockStore, azureDisk, gcepersistentDisk Distributed filesystem types - glusterfs, cephfs Special type - persistent volume, persistent volume claim Note: emptyDIR - It\u2019s a type of storage types that writes data only in memory till the pods running. So you data will be erased when the pod is deleted. So it\u2019s not a persistent kind of types. hostpath, local, fc and other types are persistent kind only, but volume won\u2019t be available across the nodes. It will be available only on local nodes. So we may need to setup something shared volume using traditional storage mount across all the nodes. Persistent volume type volumes can be accessible across all the nodes. How to use kubernetes volumes to pod and containers? Use an option \"Volumes\" along with name and types as below in a deployment file for the entire PODS and use the \"volumeMounts\" along with mountPath where the volume to be mounted for the container. We must use the volume name unique and exactly as specified in specification for the containers. If not you will end up with error. Example: spec: volumes: name: volume hostPath: path: /mnt/data containers: name: container1-nginx image: nginx volumeMounts: name: volume mountPath: \"/var/nginx-data\" name: container2-tomcat image: tomcat Above example tells that, volume name \"volume\" specified in \"spec\" section with Path \"/mnt/data\" will be used as a volume for this entire pod. It will be mounted only on container \"container1-nginx\" since it is claimed to be mounted on path \"/var/nginx-data\" using \"volumeMounts\" option. How to assign a single volume to specific container in a pod? In order to use a volume only to specific container running in a pod, we must use volume mounts option. So that particular container will use the volume specified in spec. kind: Deployment apiVersion: apps/v1 metadata: name: ebay-app spec: selector: matchLabels: environment: dev app: ebay replicas: 1 template: metadata: labels: environment: dev app: ebay spec: volumes: name: volume hostPath: path: /mnt/data containers: name: container1-nginx image: nginx volumeMounts: name: volume mountPath: \"/var/nginx-data\" name: container2-tomcat image: tomcat So we have claimed the volume name \"volume\" from specification and mapped to the container \"container1-nginx\" that would mount the volume under \"/var/nginx-data\", this volume will be only available to the first container \"container1-nginx\" not to the second container \"container2- tomcat\". This is how we can assign a single volume to specific container in a pod. How to share a same volume to all containers within a pod? In order to share a same volume to all containers running in a pod, we must use volume mounts option in all containers. kind: Deployment apiVersion: apps/v1 metadata: name: ebay-app spec: selector: matchLabels: environment: dev app: ebay replicas: 1 template: metadata: labels: environment: dev app: ebay spec: volumes: name: volume hostPath: path: /mnt/data containers: name: container1-nginx image: nginx volumeMounts: name: volume mountPath: \"/var/nginx-data\" name: container2-tomcat image: tomcat volumeMounts: name: volume mountPath: \"/var/tomcat-data\" This time, we have used volumeMount option for both containers with different path, as per the code definition, same volume \"volume\" will be mount on both containers in path \"/var/nginx- data\" on container1-nginx and \"/var/tomcat-data\" on container2-tomcat respectively. How to assign a dedicated volumes to each container in a pod? In order to assign a dedicated volumes to each containers running in a pod, we must use volumes and volumemounts option in all containers accordingly as per the example given below. kind: Deployment apiVersion: apps/v1 metadata: name: ebay-app spec: selector: matchLabels: environment: dev app: ebay replicas: 1 template: metadata: labels: environment: dev app: ebay spec: volumes: name: volume1 hostPath: path: /mnt/data1 name: volume2 hostPath: path: /mnt/data2 containers: name: container1-nginx image: nginx volumeMounts: name: volume1 mountPath: \"/var/nginx-data\" name: container2-tomcat image: tomcat volumeMounts: name: volume2 mountPath: \"/var/tomcat-data\" As per the above example, volume1 will be used by the first container \"container1-nginx\" and volume2 will be used by the second container \"container2-tomcat\". This is how we can assign dedicated volumes to each containers running in a pod. How to assign a shared volume across all pods running on different worker nodes? Why do we actually need this setup is, so far we have seen volumes that is used only on single pod running on one worker node. So your data won\u2019t be available when pod is rescheduled to other node since your hostpath you have used is local directory. If you want your data to be available for all worker nodes, we must have shared volumes concepts to overcome such situation. We can use a special type ie PersistentVolume and PersistentVolumeClaim or our traditional approach that mount a shared volume from storage and use that mounted path in the deployment file. You can checkout this video for the traditional approach and will explain you about persistentvolume and persistentvolume claim in the next article. To get all persistent volumes","title":"cat nfs_pv.yaml"},{"location":"k8-docs/#kubectl-get-persistentvolume","text":"To get all persistent volumes Claims","title":"kubectl get persistentvolume"},{"location":"k8-docs/#kubectl-get-persistentvolumeclaims-o-wide","text":"Kubernetes Security Role-Based Access Control (RBAC) In order to fully grasp the idea of RBAC, we must understand that three elements are involved: Subjects: The set of users and processes that want to access the Kubernetes API. Resources: The set of Kubernetes API Objects available in the cluster. Examples include Pods, Deployments, Services, Nodes, and PersistentVolumes, among others. Verbs: The set of operations that can be executed to the resources above. Different verbs are available (examples: get, watch, create, delete, etc.), but ultimately all of them are Create, Read, Update or Delete (CRUD) operations. With these three elements in mind, the key idea of RBAC is the following: We want to connect subjects, API resources, and operations. In other words, we want to specify, given a user , which operations can be executed over a set of resources . So, if we think about connecting these three types of entities, we can understand the different RBAC API Objects available in Kubernetes. Roles: Will connect API Resources and Verbs. These can be reused for different subjects. These are binded to one namespace (we cannot use wildcards to represent more than one, but we can deploy the same role object in different namespaces). If we want the role to be applied cluster-wide, the equivalent object is called ClusterRoles. RoleBinding: Will connect the remaining entity-subjects. Given a role, which already binds API Objects and verbs, we will establish which subjects can use it. For the cluster- level, non-namespaced equivalent, there are ClusterRoleBindings. In the example below, we are granting the user jsalmeron the ability to read, list and create pods inside the namespace test. This means that jsalmeron will be able to execute these commands: But not these: Example yaml files: Another interesting point would be the following: now that the user can create pods, can we limit how many? In order to do so, other objects, not directly related to the RBAC specification, allow configuring the amount of resources: ResourceQuota and LimitRanges . It is worth checking them out for configuring such a vital aspect of the cluster. Users and\u2026 ServiceAccounts? One topic that many Kubernetes users struggle with is the concept of subjects, but more specifically the difference between regular users and ServiceAccounts. In theory it looks simple: Users: These are global, and meant for humans or processes living outside the cluster. ServiceAccounts: These are namespaced and meant for intra-cluster processes running inside pods. Both have in common that they want to authenticate against the API in order to perform a set of operations over a set of resources (remember the previous section), and their domains seem to be clearly defined. They can also belong to what is known as groups, so a RoleBinding can bind more than one subject (but ServiceAccounts can only belong to the \u201csystem:serviceaccounts\u201d group). However, the key difference is a cause of several headaches: users do not have an associated Kubernetes API Object. That means that while this operation exists: This one doesn\u2019t: This has a vital consequence: if the cluster will not store any information about users, then, the administrator will need to manage identities outside the cluster. There are different ways to do so: TLS certificates, tokens, and OAuth2, among others. In addition, we would need to create kubectl contexts so we could access the cluster with these new credentials. In order to create the credential files, we could use the kubectl config commands (which do not require any access to the Kubernetes API, so they could be executed by any user). Watch the video above to see a complete example of user creation with TLS certificates. Possible operations over these resources are: create get delete list update edit watch exec Use case: Create user with limited namespace access In this example, we will create the following User Account: Username: employee Group: bitnami We will add the necessary RBAC policies so this user can fully manage deployments (i.e. use kubectl run command) only inside the office namespace. At the end, we will test the policies to make sure they work as expected. Step 1: Create the office namespace Execute the kubectl create command to create the namespace (as the admin user): kubectl create namespace office Step 2: Create the user credentials As previously mentioned, Kubernetes does not have API Objects for User Accounts. Of the available ways to manage authentication (see Kubernetes official documentation for a complete list), we will use OpenSSL certificates for their simplicity. The necessary steps are: Create a private key for your user. In this example, we will name the file employee.key : openssl genrsa -out employee.key 2048 Create a certificate sign request employee.csr using the private key you just created ( employee.key in this example). Make sure you specify your username and group in the - subj section (CN is for the username and O for the group). As previously mentioned, we will use employee as the name and bitnami as the group: openssl req -new -key employee.key -out employee.csr -subj \"/CN=employee/O=bitnami\" Locate your Kubernetes cluster certificate authority (CA). This will be responsible for approving the request and generating the necessary certificate to access the cluster API. Its location is normally /etc/kubernetes/pki/ . In the case of Minikube, it would be ~/.minikube/ . Check that the files ca.crt and ca.key exist in the location. Generate the final certificate employee.crt by approving the certificate sign request, employee.csr , you made earlier. Make sure you substitute the CA_LOCATION placeholder with the location of your cluster CA. In this example, the certificate will be valid for 500 days: openssl x509 -req -in employee.csr -CA CA_LOCATION/ca.crt -CAkey CA_LOCATION/ca.key -CAcreateserial -out employee.crt -days 500 Save both employee.crt and employee.key in a safe location (in this example we will use /home/employee/.certs/ ). Add a new context with the new credentials for your Kubernetes cluster. This example is for a Minikube cluster but it should be similar for others: kubectl config set-credentials employee --client- certificate=/home/employee/.certs/employee.crt --client- key=/home/employee/.certs/employee.key kubectl config set-context employee-context --cluster=minikube -- namespace=office --user=employee Now you should get an access denied error when using the kubectl CLI with this configuration file. This is expected as we have not defined any permitted operations for this user. kubectl --context=employee-context get pods Step 3: Create the role for managing deployments Create a role-deployment-manager.yaml file with the content below. In this yaml file we are creating the rule that allows a user to execute several operations on Deployments, Pods and ReplicaSets (necessary for creating a Deployment), which belong to the core (expressed by \"\" in the yaml file), apps , and extensions API Groups: kind: Role apiVersion: rbac.authorization.k8s.io/v1beta1 metadata: namespace: office name: deployment-manager rules: apiGroups: [\"\", \"extensions\", \"apps\"] resources: [\"deployments\", \"replicasets\", \"pods\"] verbs: [\"get\", \"list\", \"watch\", \"create\", \"update\", \"patch\", \"delete\"] # You can also use [\"*\"] Create the Role in the cluster using the kubectl create role command: kubectl create -f role-deployment-manager.yaml Step 4: Bind the role to the employee user Create a rolebinding-deployment-manager.yaml file with the content below. In this file, we are binding the deployment-manager Role to the User Account employee inside the office namespace: kind: RoleBinding apiVersion: rbac.authorization.k8s.io/v1beta1 metadata: name: deployment-manager-binding namespace: office subjects: kind: User name: employee apiGroup: \"\" roleRef: kind: Role name: deployment-manager apiGroup: \"\" Deploy the RoleBinding by running the kubectl create command: kubectl create -f rolebinding-deployment-manager.yaml Step 5: Test the RBAC rule Now you should be able to execute the following commands without any issues: kubectl --context=employee-context run --image bitnami/dokuwiki mydokuwiki kubectl --context=employee-context get pods If you run the same command with the --namespace=default argument, it will fail, as the employee user does not have access to this namespace. kubectl --context=employee-context get pods --namespace=default Now you have created a user with limited permissions in your cluster. Secrets Secret - At the application level, Kubernetes secrets can store sensitive information (such as passwords, SSH keys, API keys or tokens) per cluster (a virtual cluster if using namespaces, physical otherwise). Kubernetes Secret can be injected into a Pod container either as an environment variable or mounted as a file. Using Kubernetes Secrets allows us to abstract sensitive data and configuration from application deployment. Note that secrets are accessible from any pod in the same cluster. Network policies for access to pods can be defined in a deployment. A network policy specifies how pods are allowed to communicate with each other and with other network endpoints. Note that storing sensitive data in a Kubernetes Secret does not make it secure. By default, all data in Kubernetes Secrets is stored as a plaintext encoded with base64. There are multiple ways of creating secrets in Kubernetes. Creating from txt files. Creating from yaml file. Creating From Text File In order to create secrets from a text file such as user name and password, we first need to store them in a txt file and use the following command. $ kubectl create secret generic tomcat-passwd \u2013-from-file = ./username.txt \u2013 fromfile = ./.password.txt Creating From Yaml File apiVersion: v1 kind: Secret metadata: name: tomcat-pass type: Opaque data: password: username: Creating the Secret $ kubectl create \u2013f Secret.yaml secrets/tomcat-pass Using Secrets Once we have created the secrets, it can be consumed in a pod or the replication controller as \u2212 Environment Variable Volume As Environment Variable In order to use the secret as environment variable, we will use env under the spec section of pod yaml file. env: name: SECRET_USERNAME valueFrom: secretKeyRef: name: mysecret key: tomcat-pass As Volume spec: volumes: name: \"secretstest\" secret: secretName: tomcat-pass containers: image: tomcat:7.0 name: awebserver volumeMounts: mountPath: \"/tmp/mysec\" name: \"secretstest\" **Secret Configuration as Environment Variable ** apiVersion: v1 kind: ReplicationController metadata: name: appname spec: replicas: replica_count template: metadata: name: appname spec: nodeSelector: resource-group: containers: name: appname image: imagePullPolicy: Always ports: containerPort: 3000 env: -----------------------------> 1 name: ENV valueFrom: configMapKeyRef: name: appname key: tomcat-secrets In the above code, under the env definition, we are using secrets as environment variable in the replication controller. Secrets as Volume Mount apiVersion: v1 kind: pod metadata: name: appname spec: metadata: name: appname spec: volumes: name: \"secretstest\" secret: secretName: tomcat-pass containers: image: tomcat: 8.0 name: awebserver volumeMounts: mountPath: \"/tmp/mysec\" name: \"secretstest\" **Kubernetes Logging & Monitoring ** Monitoring is one of the key component for managing large clusters. For this, we have a number of tools. Monitoring with Prometheus It is a monitoring and alerting system. It was built at SoundCloud and was open sourced in 2012. It handles the multi-dimensional data very well. Sematext Docker Agent It is a modern Docker-aware metrics, events, and log collection agent. It runs as a tiny container on every Docker host and collects logs, metrics, and events for all cluster node and containers. It discovers all containers (one pod might contain multiple containers) including containers for Kubernetes core services, if the core services are deployed in Docker containers. After its deployment, all logs and metrics are immediately available out of the box. Kubernetes Log Kubernetes containers\u2019 logs are not much different from Docker container logs. However, Kubernetes users need to view logs for the deployed pods. Hence, it is very useful to have Kubernetes-specific information available for log search, such as \u2212 Kubernetes namespace Kubernetes pod name Kubernetes container name Docker image name Kubernetes UID **Resources ** https://kubernetes.io/docs/ https://wiki.aquasec.com/display/containers/Kubernetes+101 https://www.ibm.com/cloud/architecture/content/course/kubernetes-101 https://kubernetesbyexample.com/ https://www.tutorialspoint.com/kubernetes/index.htm https://unofficial-kubernetes.readthedocs.io/en/latest/ https://www.katacoda.com/courses/kubernetes https://access.redhat.com/documentation/en- us/red_hat_enterprise_linux_atomic_host/7/html/getting_started_with_kubernetes/index 71*","title":"kubectl get persistentvolumeclaims -o wide"},{"location":"kubernetes/","text":"Kubernetes Kubernetes Overview Defination Why to use K8 Architecture - Architecture Diagram - K8 workflow diagram K8 components Deployment Node Pod Service Ingress Architecture of Ingress Ingress YAML Ingress Vs Internal Service Ingress YAML for path Volumes Secret ConfigMap StatefulSet Minikube abd Kubectl-Setup K8 conf yam file K8 namespaces K8 ingress Helm Volumes-Persisting Data K8 stateful set Overview Defination Kubernetes is a portable, extensible, open source platform for managing containerized workloads and services, that facilitates both declarative configuration and automation. It has a large, rapidly growing ecosystem. Kubernetes services, support, and tools are widely available. For more information visit here Why to use K8 Containers are a good way to bundle and run your applications. In a production environment, you need to manage the containers that run the applications and ensure that there is no downtime. For example, if a container goes down, another container needs to start. Wouldn't it be easier if this behavior was handled by a system? That's how Kubernetes comes to the rescue! Kubernetes provides you with a framework to run distributed systems resiliently. It takes care of scaling and failover for your application, provides deployment patterns, and more. For example: Kubernetes can easily manage a canary deployment for your system. Kubernetes provides you with: Service discovery and load balancing Kubernetes can expose a container using the DNS name or using their own IP address. If traffic to a container is high, Kubernetes is able to load balance and distribute the network traffic so that the deployment is stable. Storage orchestration Kubernetes allows you to automatically mount a storage system of your choice, such as local storages, public cloud providers, and more. Automated rollouts and rollbacks You can describe the desired state for your deployed containers using Kubernetes, and it can change the actual state to the desired state at a controlled rate. For example, you can automate Kubernetes to create new containers for your deployment, remove existing containers and adopt all their resources to the new container. Automatic bin packing You provide Kubernetes with a cluster of nodes that it can use to run containerized tasks. You tell Kubernetes how much CPU and memory (RAM) each container needs. Kubernetes can fit containers onto your nodes to make the best use of your resources. Self-healing Kubernetes restarts containers that fail, replaces containers, kills containers that don't respond to your user-defined health check, and doesn't advertise them to clients until they are ready to serve. Secret and configuration management Kubernetes lets you store and manage sensitive information, such as passwords, OAuth tokens, and SSH keys. You can deploy and update secrets and application configuration without rebuilding your container images, and without exposing secrets in your stack configuration. Architecture - Architecture Diagram - K8 workflow diagram K8 components Deployment Node nodes Pod pods Service service Ingress ingress Architecture of Ingress Ingress YAML Ingress Vs Internal Service Ingress YAML for path Volumes Secret ConfigMap StatefulSet Minikube abd Kubectl-Setup K8 conf yam file K8 namespaces K8 ingress Helm Volumes-Persisting Data K8 stateful set","title":"Kubernetes"},{"location":"kubernetes/#kubernetes","text":"Kubernetes Overview Defination Why to use K8 Architecture - Architecture Diagram - K8 workflow diagram K8 components Deployment Node Pod Service Ingress Architecture of Ingress Ingress YAML Ingress Vs Internal Service Ingress YAML for path Volumes Secret ConfigMap StatefulSet Minikube abd Kubectl-Setup K8 conf yam file K8 namespaces K8 ingress Helm Volumes-Persisting Data K8 stateful set","title":"Kubernetes"},{"location":"kubernetes/#overview","text":"","title":"Overview"},{"location":"kubernetes/#defination","text":"Kubernetes is a portable, extensible, open source platform for managing containerized workloads and services, that facilitates both declarative configuration and automation. It has a large, rapidly growing ecosystem. Kubernetes services, support, and tools are widely available. For more information visit here","title":"Defination"},{"location":"kubernetes/#why-to-use-k8","text":"Containers are a good way to bundle and run your applications. In a production environment, you need to manage the containers that run the applications and ensure that there is no downtime. For example, if a container goes down, another container needs to start. Wouldn't it be easier if this behavior was handled by a system? That's how Kubernetes comes to the rescue! Kubernetes provides you with a framework to run distributed systems resiliently. It takes care of scaling and failover for your application, provides deployment patterns, and more. For example: Kubernetes can easily manage a canary deployment for your system. Kubernetes provides you with: Service discovery and load balancing Kubernetes can expose a container using the DNS name or using their own IP address. If traffic to a container is high, Kubernetes is able to load balance and distribute the network traffic so that the deployment is stable. Storage orchestration Kubernetes allows you to automatically mount a storage system of your choice, such as local storages, public cloud providers, and more. Automated rollouts and rollbacks You can describe the desired state for your deployed containers using Kubernetes, and it can change the actual state to the desired state at a controlled rate. For example, you can automate Kubernetes to create new containers for your deployment, remove existing containers and adopt all their resources to the new container. Automatic bin packing You provide Kubernetes with a cluster of nodes that it can use to run containerized tasks. You tell Kubernetes how much CPU and memory (RAM) each container needs. Kubernetes can fit containers onto your nodes to make the best use of your resources. Self-healing Kubernetes restarts containers that fail, replaces containers, kills containers that don't respond to your user-defined health check, and doesn't advertise them to clients until they are ready to serve. Secret and configuration management Kubernetes lets you store and manage sensitive information, such as passwords, OAuth tokens, and SSH keys. You can deploy and update secrets and application configuration without rebuilding your container images, and without exposing secrets in your stack configuration.","title":"Why to use K8"},{"location":"kubernetes/#architecture","text":"","title":"Architecture"},{"location":"kubernetes/#-architecture-diagram","text":"","title":"- Architecture Diagram"},{"location":"kubernetes/#-k8-workflow-diagram","text":"","title":"- K8 workflow diagram"},{"location":"kubernetes/#k8-components","text":"","title":"K8 components"},{"location":"kubernetes/#deployment","text":"","title":"Deployment"},{"location":"kubernetes/#node","text":"nodes","title":"Node"},{"location":"kubernetes/#pod","text":"pods","title":"Pod"},{"location":"kubernetes/#service","text":"service","title":"Service"},{"location":"kubernetes/#ingress","text":"ingress","title":"Ingress"},{"location":"kubernetes/#architecture-of-ingress","text":"","title":"Architecture of Ingress"},{"location":"kubernetes/#ingress-yaml","text":"","title":"Ingress YAML"},{"location":"kubernetes/#ingress-vs-internal-service","text":"","title":"Ingress Vs Internal Service"},{"location":"kubernetes/#ingress-yaml-for-path","text":"","title":"Ingress YAML for path"},{"location":"kubernetes/#volumes","text":"","title":"Volumes"},{"location":"kubernetes/#secret","text":"","title":"Secret"},{"location":"kubernetes/#configmap","text":"","title":"ConfigMap"},{"location":"kubernetes/#statefulset","text":"","title":"StatefulSet"},{"location":"kubernetes/#minikube-abd-kubectl-setup","text":"","title":"Minikube abd Kubectl-Setup"},{"location":"kubernetes/#k8-conf-yam-file","text":"","title":"K8 conf yam file"},{"location":"kubernetes/#k8-namespaces","text":"","title":"K8 namespaces"},{"location":"kubernetes/#k8-ingress","text":"","title":"K8 ingress"},{"location":"kubernetes/#helm","text":"","title":"Helm"},{"location":"kubernetes/#volumes-persisting-data","text":"","title":"Volumes-Persisting Data"},{"location":"kubernetes/#k8-stateful-set","text":"","title":"K8 stateful set"},{"location":"linux/","text":"Document for Linux Directory usage: Linux folder usage Chmod drwxrwxrwx > rwx abbrevates as Read, Write , Delete/Execute Chmod is use for Modify permission of file/directory. syntax chmod 777 \"file-name/folder-name\" Chown is use for providing permissions or changing the ownership to other User syntax to change Owner/User chown \"user-name\" \"file/folder-name\" to change Owner and Group chown \"user-name\":\"group-name\" \"file/folder-name\" Add User to add user in linux syntax sudo adduser <user-name> #it will ask for password enter new password switch to new user syntax su - <user-name> after login with new user try to execute apt-get update #you may be get error as incident will be reported. update the permissions for new user exit #logout from the new user for the root user sudo visudo update members of the admin group %<username> ALL=(ALL) NOPASSWD:ALL #add this line It contains Owner, Group, Public chmod calculator Linux commands Execute Command for Help command man \"ls\", man \"mkdir\" Change directory cd List directory contents ls Print working directory pwd Create a new directory mkdir \"dir-name\" Remove an empty directory rmdir \"dir-name\" Delete directory rm -r \"dir-name\" Remove files rm \"file-name\" Concatenate and display files cat \"file-name\" Create an empty file touch \"file-name\" create file with adding content vim \"file-name\" Vim command to write use i as insert, to save ESC : wq , to save without editing ESC : q! Move or rename files and directories mv \"file-name\" \"dir-name\"/\"file-name\" Copy file to new file cp \"file-name1\" \"file-name2\" Copy file to other folder cp \"file-name1\" \"dir-name\"/\"file-name1\" Modify Permission of file/dir chmod \"777\" \"file/dir-name\" Change ownership of file/dir chown \"user-name\" \"file/dir-name\" Change ownership & group for file/dir chown \"user-name\":\"grp-name\" \"file/dir-name\" Tar Commands Execute Command for Help command man \"tar\" extract files x list the contents of archive t append files to existing archive r use gzip compression z use bzip2 compression j create tar file tar cf \"file-name.tar\" \"file/directory-name\" extract tar file tar -xvf \"file-name.tar\" -v: contents of tar, -f: tar filename compress files gzip \"fil-ename\" decompress files gunzip \"file-name\" to update system sudo apt update Advance Commands Execute Command send file to other system using IP add. scp \"file-path\" root@IPofothersystem:/path check network connectivity ping google.com display network configure ifconfig display network connection netstat control system service & settings systemctl to start nginx systemctl start nginx systemctl status nginx systemctl stop nginx add new user useradd \"user-name\" change password for user passwd \"user-name\" switch user su check current user whoami system Disk usage df mount file system sudo mount /dev/folder /mnt/folder check packages or app install path which cmake save terminal output to txt file touch file.txt ls | tee file.txt Check cPU, RAM usage htop |cut | Cut out sections of a file |gzip | Compress or decompress files using gzip |gunzip | Decompress files compressed with gzip |find | Find files and directories matching a pattern |grep | Search for a pattern in a file |awk | Pattern scanning and processing language |sed | Stream editor for filtering and transforming text |head | Display the first few lines of a file |tail | Display the last few lines of a file |sort | Sort lines of a file |uniq | Remove duplicate lines from a file |wc | Count lines, words, and characters in a file |diff | Compare two files line by line |patch | Apply a patch to a file |chmod | Change permissions of files and directories |chown | Change the owner of a file or directory |chgrp | Change the group ownership of a file or directory |ps | List running processes |top | Display system resource usage and process information |kill | Send a signal to a process to terminate it |du | Display disk usage of files and directories |df | Display free disk space on the file system |mount | Mount a file system |umount | Unmount a file system |ping | Test connectivity to a network host |ssh | Secure shell remote login and command execution |scp | Secure copy files between hosts |rsync | Remote file and directory synchronization |curl | Transfer data from or to a server using various protocols |wget | Retrieve files from the web using various protocols |ftp | File Transfer Protocol client |sftp | Secure File Transfer Protocol client |telnet | Telnet client |nslookup | DNS lookup utility |dig | DNS lookup utility |netstat | Display network connections and statistics |ifconfig | Configure network interfaces |route | Display or modify the routing table |iptables | Firewall and packet filtering utility |hostname | Display or set the hostname of the system |date | Display or set the system date and time |timedatectl | Control the system date and time |uname | Display system information |whoami | Display the current user ID |id | Display user and group information |su | Switch user to become another user |sudo | Execute a command with superuser privileges |passwd | Change the password of a user account |useradd | Create a new user account |userdel | Delete a user account |usermod | Modify a user account |groupadd | Create a new group |groupdel | Delete a group |groupmod | Modify a group |finger | Display information about users on the system |last | Display information about recent logins history \u2014 Display command history echo \u2014 Print a message to the terminal printf \u2014 Format and print data lshw \u2014 Displays hardware information lspci \u2014 Displays information about PCI buses and devices. lsusb \u2014 Displays information about USB buses and devices. hwinfo \u2014 Displays detailed hardware information. free \u2014 Displays memory usage. vmstat \u2014 Displays system memory, processor, and I/O statistics. iostat \u2014 Displays CPU and disk I/O statistics. uptime \u2014 Displays system uptime and load averages. journalctl \u2014 Displays the system journal. dmesg \u2014 Displays the kernel ring buffer. crontab \u2014 Schedules recurring tasks. at \u2014 Schedules a one-time task. service \u2014 Manages system services. systemctl \u2014 Controls system services in systemd-based distributions. traceroute \u2014 Traces the network path to a remote host. bzip2 \u2014 Compresses files using the bzip2 algorithm. unzip \u2014 Extracts files from a ZIP archive. tee \u2014 Redirect output to multiple files chroot \u2014 Change the root directory for a process ps aux \u2014 Display information about all running processes less \u2014 Display file contents in a paginated format more \u2014 Display file contents one page at a time ln \u2014 Create links between files realpath \u2014 Print the resolved absolute path of a file watch \u2014 Execute a command periodically and display the output cal \u2014 Display a calendar tar -xzvf \u2014 Extract files from a compressed archive tar -czvf \u2014 Create a compressed archive whereis \u2014 Locate the binary, source, and manual page files for a command locate \u2014 Find files by name which \u2014 Display the full path to an executable","title":"Linux Cmd"},{"location":"linux/#document-for-linux-directory-usage","text":"Linux folder usage","title":"Document for Linux Directory usage:"},{"location":"linux/#chmod","text":"drwxrwxrwx > rwx abbrevates as Read, Write , Delete/Execute Chmod is use for Modify permission of file/directory. syntax chmod 777 \"file-name/folder-name\" Chown is use for providing permissions or changing the ownership to other User syntax to change Owner/User chown \"user-name\" \"file/folder-name\" to change Owner and Group chown \"user-name\":\"group-name\" \"file/folder-name\"","title":"Chmod"},{"location":"linux/#add-user","text":"to add user in linux syntax sudo adduser <user-name> #it will ask for password enter new password switch to new user syntax su - <user-name> after login with new user try to execute apt-get update #you may be get error as incident will be reported. update the permissions for new user exit #logout from the new user for the root user sudo visudo update members of the admin group %<username> ALL=(ALL) NOPASSWD:ALL #add this line It contains Owner, Group, Public","title":"Add User"},{"location":"linux/#chmod-calculator","text":"","title":"chmod calculator"},{"location":"linux/#linux-commands","text":"Execute Command for Help command man \"ls\", man \"mkdir\" Change directory cd List directory contents ls Print working directory pwd Create a new directory mkdir \"dir-name\" Remove an empty directory rmdir \"dir-name\" Delete directory rm -r \"dir-name\" Remove files rm \"file-name\" Concatenate and display files cat \"file-name\" Create an empty file touch \"file-name\" create file with adding content vim \"file-name\" Vim command to write use i as insert, to save ESC : wq , to save without editing ESC : q! Move or rename files and directories mv \"file-name\" \"dir-name\"/\"file-name\" Copy file to new file cp \"file-name1\" \"file-name2\" Copy file to other folder cp \"file-name1\" \"dir-name\"/\"file-name1\" Modify Permission of file/dir chmod \"777\" \"file/dir-name\" Change ownership of file/dir chown \"user-name\" \"file/dir-name\" Change ownership & group for file/dir chown \"user-name\":\"grp-name\" \"file/dir-name\"","title":"Linux commands"},{"location":"linux/#tar-commands","text":"Execute Command for Help command man \"tar\" extract files x list the contents of archive t append files to existing archive r use gzip compression z use bzip2 compression j create tar file tar cf \"file-name.tar\" \"file/directory-name\" extract tar file tar -xvf \"file-name.tar\" -v: contents of tar, -f: tar filename compress files gzip \"fil-ename\" decompress files gunzip \"file-name\" to update system sudo apt update","title":"Tar Commands"},{"location":"linux/#advance-commands","text":"Execute Command send file to other system using IP add. scp \"file-path\" root@IPofothersystem:/path check network connectivity ping google.com display network configure ifconfig display network connection netstat control system service & settings systemctl to start nginx systemctl start nginx systemctl status nginx systemctl stop nginx add new user useradd \"user-name\" change password for user passwd \"user-name\" switch user su check current user whoami system Disk usage df mount file system sudo mount /dev/folder /mnt/folder check packages or app install path which cmake save terminal output to txt file touch file.txt ls | tee file.txt Check cPU, RAM usage htop |cut | Cut out sections of a file |gzip | Compress or decompress files using gzip |gunzip | Decompress files compressed with gzip |find | Find files and directories matching a pattern |grep | Search for a pattern in a file |awk | Pattern scanning and processing language |sed | Stream editor for filtering and transforming text |head | Display the first few lines of a file |tail | Display the last few lines of a file |sort | Sort lines of a file |uniq | Remove duplicate lines from a file |wc | Count lines, words, and characters in a file |diff | Compare two files line by line |patch | Apply a patch to a file |chmod | Change permissions of files and directories |chown | Change the owner of a file or directory |chgrp | Change the group ownership of a file or directory |ps | List running processes |top | Display system resource usage and process information |kill | Send a signal to a process to terminate it |du | Display disk usage of files and directories |df | Display free disk space on the file system |mount | Mount a file system |umount | Unmount a file system |ping | Test connectivity to a network host |ssh | Secure shell remote login and command execution |scp | Secure copy files between hosts |rsync | Remote file and directory synchronization |curl | Transfer data from or to a server using various protocols |wget | Retrieve files from the web using various protocols |ftp | File Transfer Protocol client |sftp | Secure File Transfer Protocol client |telnet | Telnet client |nslookup | DNS lookup utility |dig | DNS lookup utility |netstat | Display network connections and statistics |ifconfig | Configure network interfaces |route | Display or modify the routing table |iptables | Firewall and packet filtering utility |hostname | Display or set the hostname of the system |date | Display or set the system date and time |timedatectl | Control the system date and time |uname | Display system information |whoami | Display the current user ID |id | Display user and group information |su | Switch user to become another user |sudo | Execute a command with superuser privileges |passwd | Change the password of a user account |useradd | Create a new user account |userdel | Delete a user account |usermod | Modify a user account |groupadd | Create a new group |groupdel | Delete a group |groupmod | Modify a group |finger | Display information about users on the system |last | Display information about recent logins history \u2014 Display command history echo \u2014 Print a message to the terminal printf \u2014 Format and print data lshw \u2014 Displays hardware information lspci \u2014 Displays information about PCI buses and devices. lsusb \u2014 Displays information about USB buses and devices. hwinfo \u2014 Displays detailed hardware information. free \u2014 Displays memory usage. vmstat \u2014 Displays system memory, processor, and I/O statistics. iostat \u2014 Displays CPU and disk I/O statistics. uptime \u2014 Displays system uptime and load averages. journalctl \u2014 Displays the system journal. dmesg \u2014 Displays the kernel ring buffer. crontab \u2014 Schedules recurring tasks. at \u2014 Schedules a one-time task. service \u2014 Manages system services. systemctl \u2014 Controls system services in systemd-based distributions. traceroute \u2014 Traces the network path to a remote host. bzip2 \u2014 Compresses files using the bzip2 algorithm. unzip \u2014 Extracts files from a ZIP archive. tee \u2014 Redirect output to multiple files chroot \u2014 Change the root directory for a process ps aux \u2014 Display information about all running processes less \u2014 Display file contents in a paginated format more \u2014 Display file contents one page at a time ln \u2014 Create links between files realpath \u2014 Print the resolved absolute path of a file watch \u2014 Execute a command periodically and display the output cal \u2014 Display a calendar tar -xzvf \u2014 Extract files from a compressed archive tar -czvf \u2014 Create a compressed archive whereis \u2014 Locate the binary, source, and manual page files for a command locate \u2014 Find files by name which \u2014 Display the full path to an executable","title":"Advance Commands"},{"location":"networking/","text":"Computer Networking Computer Networking Protocol: TCP (Transmission Control Protocol): HTTP (Hypertext Transfer Protocol): UDP (User Datagram Protocol): Router Port Number: Internet speed OSI Model Physical Layer (Layer 1): Data Link Layer (Layer 2): Network Layer (Layer 3): Transport Layer (Layer 4): Session Layer (Layer 5): Presentation Layer (Layer 6): Application Layer (Layer 7): TCP/IP Application Layer: Transport Layer: Internet Layer: Link Layer: OSI vs TCP/IP HTTPS Request HTTPS Status Code SMTP: (Sender Mail Transfer Protocol) Delete local host running backend using netstat Protocol: Protocol refers to a set of rules and conventions that define how data is transmitted, received, and processed between devices and systems connected to the network. These rules ensure that devices can understand and communicate with each other effectively, enabling the seamless exchange of information across the internet. TCP (Transmission Control Protocol): Imagine sending a letter through the postal service. TCP is like a reliable, organized courier service that guarantees the letter's delivery. TCP ensures that data is transmitted in a precise, ordered manner, and it makes sure that the data arrives without errors. If a packet of data is lost or damaged during transmission, TCP requests the missing data to be sent again until everything is received correctly. It is commonly used for applications that require accurate data delivery, like web pages, emails, and file downloads. HTTP (Hypertext Transfer Protocol): HTTP is like a set of rules that web browsers and web servers follow to communicate with each other. It's the language they use to exchange web pages and other resources. When you type a website address (URL) into your browser and hit Enter, your browser sends an HTTP request to the web server asking for the webpage. The server responds with the requested webpage, and your browser displays it for you to see. HTTP is the foundation of the World Wide Web, allowing us to access websites and navigate the internet. UDP (User Datagram Protocol): UDP is like a fast, simple, but less reliable delivery service. It's useful for scenarios where speed is more important than guaranteed delivery. Unlike TCP, UDP doesn't ensure that data arrives in order or without errors. It just sends the data as quickly as possible. If some data packets are lost during transmission, UDP does not request retransmission. It is commonly used for real-time applications like video streaming, online gaming, and VoIP (Voice over Internet Protocol), where slight delays are acceptable and retransmission might cause more issues than it solves. Router Every Router/Modem has Global IP address, this IP address is shared to each device(computer/mobile) that connected to Router/Modem. for eg: If you run multiple applications as google, whatsapp and send request to internet, the device configure to send back response to particular Application using Port number. In simple IP address is to figureout which device/router you are using and Port number is to find from which Application response need to send. Port Number: Well-Known Ports (0-1023): Port numbers from 0 to 1023 are reserved for well-known services. Many of these ports are standardized for specific applications and protocols. For example, port 80 is commonly used for HTTP, port 443 for HTTPS, and port 22 for SSH. Registered Ports (1024-49151): Port numbers from 1024 to 49151 are assigned to registered services. These ports are used for various applications and services, and some have been officially registered with the Internet Assigned Numbers Authority (IANA). for eg: SQL = 1433 MongoDB = 27017 Dynamic/Private Ports (49152-65535): Port numbers from 49152 to 65535 are considered dynamic or private ports. They are used for temporary or private purposes, and they are less likely to be officially registered with IANA. Internet speed OSI Model The OSI (Open Systems Interconnection) model is a conceptual framework that standardizes the functions of a telecommunication or computing system into seven distinct layers. The model was developed by the International Organization for Standardization (ISO) to promote interoperability and facilitate communication between different systems and devices. Each layer in the OSI model represents a specific set of functions and services, and it helps in understanding and troubleshooting network communication. The seven layers of the OSI model, from the bottom to the top, are as follows: Physical Layer (Layer 1): The physical layer is responsible for transmitting raw bits over a physical medium, such as cables or wireless signals. It defines the electrical, mechanical, and procedural aspects of data transmission, such as voltage levels, cable types, and physical connectors. Data Link Layer (Layer 2): The data link layer is responsible for reliable data transfer between directly connected devices on the same network segment. It provides error detection and correction, and it ensures that data frames are transmitted and received correctly. Network Layer (Layer 3): The network layer is responsible for routing data packets between different networks. It handles logical addressing, like IP addresses, and determines the best path for data to reach its destination. Transport Layer (Layer 4): The transport layer provides end-to-end communication between devices on different networks. It ensures reliable data delivery, flow control, and error recovery using protocols like TCP (Transmission Control Protocol) and UDP (User Datagram Protocol). Session Layer (Layer 5): The session layer establishes, maintains, and terminates connections (sessions) between applications on different devices. It manages dialogues between applications, synchronization, and checkpointing for data exchange. Presentation Layer (Layer 6): The presentation layer is responsible for data representation and encryption/decryption if needed. It translates data between the application format and the network format, ensuring that both systems can understand each other. Application Layer (Layer 7): The application layer is the topmost layer that directly interacts with user applications and services. It provides network services to end-users, such as email, file transfer, web browsing, and other network applications. TCP/IP The TCP/IP model, also known as the Internet Protocol Suite, is a conceptual framework used to understand and implement network communication in the context of the internet. It was developed by the United States Department of Defense and is the foundation of the modern internet and many other computer networks. The TCP/IP model consists of four layers, each with specific functionalities: Application Layer: The Application Layer is the topmost layer and directly interacts with user applications and services. It provides network services to end-users and enables applications to communicate with the network. Protocols at this layer include HTTP (for web browsing), SMTP (for email), FTP (for file transfer), and DNS (for domain name resolution). Transport Layer: The Transport Layer is responsible for end-to-end communication between devices on different networks. It ensures reliable data delivery, flow control, and error recovery. Two main protocols at this layer are TCP (Transmission Control Protocol) and UDP (User Datagram Protocol). TCP provides reliable and ordered data delivery, while UDP is used for faster but less reliable data transmission. Internet Layer: The Internet Layer is responsible for routing data packets between different networks. It handles logical addressing, such as IP addresses, and determines the best path for data to reach its destination. The core protocol at this layer is IP (Internet Protocol), which provides the unique addressing scheme necessary for devices to communicate across the internet. Link Layer: The Link Layer, also known as the Network Interface Layer or Data Link Layer, is responsible for reliable data transfer between directly connected devices on the same network segment. It defines how data frames are transmitted and received over the physical medium, such as Ethernet or Wi-Fi. Various technologies and protocols, such as Ethernet, Wi-Fi (IEEE 802.11), and PPP (Point-to-Point Protocol), operate at this layer. OSI vs TCP/IP HTTPS Request HTTPS Status Code SMTP: (Sender Mail Transfer Protocol) Delete local host running backend using netstat netstat -ano | findstr 9090 netstat -a -o | find \"9090\" taskkill /F /PID 12345","title":"Computer Networking"},{"location":"networking/#computer-networking","text":"Computer Networking Protocol: TCP (Transmission Control Protocol): HTTP (Hypertext Transfer Protocol): UDP (User Datagram Protocol): Router Port Number: Internet speed OSI Model Physical Layer (Layer 1): Data Link Layer (Layer 2): Network Layer (Layer 3): Transport Layer (Layer 4): Session Layer (Layer 5): Presentation Layer (Layer 6): Application Layer (Layer 7): TCP/IP Application Layer: Transport Layer: Internet Layer: Link Layer: OSI vs TCP/IP HTTPS Request HTTPS Status Code SMTP: (Sender Mail Transfer Protocol) Delete local host running backend using netstat","title":"Computer Networking"},{"location":"networking/#protocol","text":"Protocol refers to a set of rules and conventions that define how data is transmitted, received, and processed between devices and systems connected to the network. These rules ensure that devices can understand and communicate with each other effectively, enabling the seamless exchange of information across the internet.","title":"Protocol:"},{"location":"networking/#tcp-transmission-control-protocol","text":"Imagine sending a letter through the postal service. TCP is like a reliable, organized courier service that guarantees the letter's delivery. TCP ensures that data is transmitted in a precise, ordered manner, and it makes sure that the data arrives without errors. If a packet of data is lost or damaged during transmission, TCP requests the missing data to be sent again until everything is received correctly. It is commonly used for applications that require accurate data delivery, like web pages, emails, and file downloads.","title":"TCP (Transmission Control Protocol):"},{"location":"networking/#http-hypertext-transfer-protocol","text":"HTTP is like a set of rules that web browsers and web servers follow to communicate with each other. It's the language they use to exchange web pages and other resources. When you type a website address (URL) into your browser and hit Enter, your browser sends an HTTP request to the web server asking for the webpage. The server responds with the requested webpage, and your browser displays it for you to see. HTTP is the foundation of the World Wide Web, allowing us to access websites and navigate the internet.","title":"HTTP (Hypertext Transfer Protocol):"},{"location":"networking/#udp-user-datagram-protocol","text":"UDP is like a fast, simple, but less reliable delivery service. It's useful for scenarios where speed is more important than guaranteed delivery. Unlike TCP, UDP doesn't ensure that data arrives in order or without errors. It just sends the data as quickly as possible. If some data packets are lost during transmission, UDP does not request retransmission. It is commonly used for real-time applications like video streaming, online gaming, and VoIP (Voice over Internet Protocol), where slight delays are acceptable and retransmission might cause more issues than it solves.","title":"UDP (User Datagram Protocol):"},{"location":"networking/#router","text":"Every Router/Modem has Global IP address, this IP address is shared to each device(computer/mobile) that connected to Router/Modem. for eg: If you run multiple applications as google, whatsapp and send request to internet, the device configure to send back response to particular Application using Port number. In simple IP address is to figureout which device/router you are using and Port number is to find from which Application response need to send.","title":"Router"},{"location":"networking/#port-number","text":"Well-Known Ports (0-1023): Port numbers from 0 to 1023 are reserved for well-known services. Many of these ports are standardized for specific applications and protocols. For example, port 80 is commonly used for HTTP, port 443 for HTTPS, and port 22 for SSH. Registered Ports (1024-49151): Port numbers from 1024 to 49151 are assigned to registered services. These ports are used for various applications and services, and some have been officially registered with the Internet Assigned Numbers Authority (IANA). for eg: SQL = 1433 MongoDB = 27017 Dynamic/Private Ports (49152-65535): Port numbers from 49152 to 65535 are considered dynamic or private ports. They are used for temporary or private purposes, and they are less likely to be officially registered with IANA.","title":"Port Number:"},{"location":"networking/#internet-speed","text":"","title":"Internet speed"},{"location":"networking/#osi-model","text":"The OSI (Open Systems Interconnection) model is a conceptual framework that standardizes the functions of a telecommunication or computing system into seven distinct layers. The model was developed by the International Organization for Standardization (ISO) to promote interoperability and facilitate communication between different systems and devices. Each layer in the OSI model represents a specific set of functions and services, and it helps in understanding and troubleshooting network communication. The seven layers of the OSI model, from the bottom to the top, are as follows:","title":"OSI Model"},{"location":"networking/#physical-layer-layer-1","text":"The physical layer is responsible for transmitting raw bits over a physical medium, such as cables or wireless signals. It defines the electrical, mechanical, and procedural aspects of data transmission, such as voltage levels, cable types, and physical connectors.","title":"Physical Layer (Layer 1):"},{"location":"networking/#data-link-layer-layer-2","text":"The data link layer is responsible for reliable data transfer between directly connected devices on the same network segment. It provides error detection and correction, and it ensures that data frames are transmitted and received correctly.","title":"Data Link Layer (Layer 2):"},{"location":"networking/#network-layer-layer-3","text":"The network layer is responsible for routing data packets between different networks. It handles logical addressing, like IP addresses, and determines the best path for data to reach its destination.","title":"Network Layer (Layer 3):"},{"location":"networking/#transport-layer-layer-4","text":"The transport layer provides end-to-end communication between devices on different networks. It ensures reliable data delivery, flow control, and error recovery using protocols like TCP (Transmission Control Protocol) and UDP (User Datagram Protocol).","title":"Transport Layer (Layer 4):"},{"location":"networking/#session-layer-layer-5","text":"The session layer establishes, maintains, and terminates connections (sessions) between applications on different devices. It manages dialogues between applications, synchronization, and checkpointing for data exchange.","title":"Session Layer (Layer 5):"},{"location":"networking/#presentation-layer-layer-6","text":"The presentation layer is responsible for data representation and encryption/decryption if needed. It translates data between the application format and the network format, ensuring that both systems can understand each other.","title":"Presentation Layer (Layer 6):"},{"location":"networking/#application-layer-layer-7","text":"The application layer is the topmost layer that directly interacts with user applications and services. It provides network services to end-users, such as email, file transfer, web browsing, and other network applications.","title":"Application Layer (Layer 7):"},{"location":"networking/#tcpip","text":"The TCP/IP model, also known as the Internet Protocol Suite, is a conceptual framework used to understand and implement network communication in the context of the internet. It was developed by the United States Department of Defense and is the foundation of the modern internet and many other computer networks. The TCP/IP model consists of four layers, each with specific functionalities:","title":"TCP/IP"},{"location":"networking/#application-layer","text":"The Application Layer is the topmost layer and directly interacts with user applications and services. It provides network services to end-users and enables applications to communicate with the network. Protocols at this layer include HTTP (for web browsing), SMTP (for email), FTP (for file transfer), and DNS (for domain name resolution).","title":"Application Layer:"},{"location":"networking/#transport-layer","text":"The Transport Layer is responsible for end-to-end communication between devices on different networks. It ensures reliable data delivery, flow control, and error recovery. Two main protocols at this layer are TCP (Transmission Control Protocol) and UDP (User Datagram Protocol). TCP provides reliable and ordered data delivery, while UDP is used for faster but less reliable data transmission.","title":"Transport Layer:"},{"location":"networking/#internet-layer","text":"The Internet Layer is responsible for routing data packets between different networks. It handles logical addressing, such as IP addresses, and determines the best path for data to reach its destination. The core protocol at this layer is IP (Internet Protocol), which provides the unique addressing scheme necessary for devices to communicate across the internet.","title":"Internet Layer:"},{"location":"networking/#link-layer","text":"The Link Layer, also known as the Network Interface Layer or Data Link Layer, is responsible for reliable data transfer between directly connected devices on the same network segment. It defines how data frames are transmitted and received over the physical medium, such as Ethernet or Wi-Fi. Various technologies and protocols, such as Ethernet, Wi-Fi (IEEE 802.11), and PPP (Point-to-Point Protocol), operate at this layer.","title":"Link Layer:"},{"location":"networking/#osi-vs-tcpip","text":"","title":"OSI vs TCP/IP"},{"location":"networking/#https-request","text":"","title":"HTTPS Request"},{"location":"networking/#https-status-code","text":"","title":"HTTPS Status Code"},{"location":"networking/#smtp-sender-mail-transfer-protocol","text":"","title":"SMTP: (Sender Mail Transfer Protocol)"},{"location":"networking/#delete-local-host-running-backend-using-netstat","text":"netstat -ano | findstr 9090 netstat -a -o | find \"9090\" taskkill /F /PID 12345","title":"Delete local host running backend using netstat"},{"location":"packer/","text":"Packer Packer What is Packer? Stages of Packer Usage of Packer Mutable and Immutable stage Mutable Immutable What is Packer? Packer is a tools which help to create customize Image from multiple platform from a single source configuration. Stages of Packer Usage of Packer Mutable and Immutable stage WHY to use PACKER.???? Well there are to stages of create Images > Mutable and Immutable Mutable means changing Continuosly. Immutable means needs to configure only one time. Mutable is old way to configure the Images. Where it needs to cofingure after deploying the application If any case, we want to deploy to multiple server, configure multiplt server individually may create new bugs. Where as Packer use Immutable, which is configure deploy deplying to server. Using single configure Image we can spin up multiple server. Mutable DEPLOY > SERVER > CONFIGURE Configuring after spinning up server, If any case we need to install dependency into that server we need to isntall it each individual server, which can lead to issues and Bugs. Immutable DEPLOY > CONFIGURE > SERVER In Immutable Deploying and Configuration is done before hosting to server In Immutable using One Packer we can spin up multiple server YT link","title":"Packer"},{"location":"packer/#packer","text":"Packer What is Packer? Stages of Packer Usage of Packer Mutable and Immutable stage Mutable Immutable","title":"Packer"},{"location":"packer/#what-is-packer","text":"Packer is a tools which help to create customize Image from multiple platform from a single source configuration.","title":"What is Packer?"},{"location":"packer/#stages-of-packer","text":"","title":"Stages of Packer"},{"location":"packer/#usage-of-packer","text":"","title":"Usage of Packer"},{"location":"packer/#mutable-and-immutable-stage","text":"WHY to use PACKER.???? Well there are to stages of create Images > Mutable and Immutable Mutable means changing Continuosly. Immutable means needs to configure only one time. Mutable is old way to configure the Images. Where it needs to cofingure after deploying the application If any case, we want to deploy to multiple server, configure multiplt server individually may create new bugs. Where as Packer use Immutable, which is configure deploy deplying to server. Using single configure Image we can spin up multiple server.","title":"Mutable and Immutable stage"},{"location":"packer/#mutable","text":"DEPLOY > SERVER > CONFIGURE Configuring after spinning up server, If any case we need to install dependency into that server we need to isntall it each individual server, which can lead to issues and Bugs.","title":"Mutable"},{"location":"packer/#immutable","text":"DEPLOY > CONFIGURE > SERVER In Immutable Deploying and Configuration is done before hosting to server In Immutable using One Packer we can spin up multiple server YT link","title":"Immutable"},{"location":"tools/","text":"Cheat Code For Tools Git git basic git config --global user.email MY_NAME@example.com git config --global user.name \"MY_NAME\" git clone {url} git branch -a (list of branch) git checkout {branch name} git add . git status git commit -am \"UPD\" git push origin <branch name> fetch git user name git config user.name git config user.email git create new branch name git checkout -b <branch_name> undo add git reset uncommit git reset HEAD^ git push git push origin <branch name> git push -f origin <branch name> git fetch for pushing into new repositories git add . git commit -am \"UPD\" git branch {branch name} ##create new branch git remote add origin {gitcloneurl} ##add git remote url ##add git credentials git push origin {branch name} merge branch vscode > git clone url git checkout main branch git pull git checkout feature branch git merge main_branch ## if you want to update feature branch with main ## you will get merge conflicts in vscode ## resolve merge conflicts git commit ##check branch name and commit to your feature branch git push feature branch rebase git rebase {branch name} show commit history git log git log -p -1 #to show last commit to delete branch git branch --delete <branchname> git using python get current branch name os.system('git rev-parse --abbrev-ref HEAD') get repository name os.system(\"git remote get-url origin\") git commit without verify git commit \u2013no-verify \u2013m \"message\" Docker pull images docker pull <image name> display images docker images display all containers docker ps -a run container docker run -it <repository-name> bash work on container docker exec -it <container_id> bash stop container docker stop <container_id> remove container docker rm <container_id> delete all containers docker container prune -f``` - kill container docker kill - Stop all the containers ```docker stop $(docker ps -a -q)``` - Remove all the containers ```docker rm $(docker ps -a -q)``` - check containers with runnning docker ps docker container ls - to remove image docker container ls docker rmi docker image rm - Remove all unused containers, volumes, networks and images docker system prune -a --volumes - Docker inspect container docker inspect - to push images in docker hub docker container ls docker commit imuser/ansible-server(any iimage name) docker images( to chechk) docker login docker push imuser/ansible-server(we can give any iimage name) ## Python ### Create Env - create virtual environment runnning python -m venv venv - start environment based on your command line tool, e.g. .\\venv\\Scripts\\activate - check installed modules pip list - create requirement.txt file pip freeze > requirements.txt - install all relevant modules via pip pip install -r requirements.txt - install python package for local development and usage: python -m pip install -e . ```","title":"Tools"},{"location":"tools/#cheat-code-for-tools","text":"","title":"Cheat Code For Tools"},{"location":"tools/#git","text":"git basic git config --global user.email MY_NAME@example.com git config --global user.name \"MY_NAME\" git clone {url} git branch -a (list of branch) git checkout {branch name} git add . git status git commit -am \"UPD\" git push origin <branch name> fetch git user name git config user.name git config user.email git create new branch name git checkout -b <branch_name> undo add git reset uncommit git reset HEAD^ git push git push origin <branch name> git push -f origin <branch name> git fetch for pushing into new repositories git add . git commit -am \"UPD\" git branch {branch name} ##create new branch git remote add origin {gitcloneurl} ##add git remote url ##add git credentials git push origin {branch name} merge branch vscode > git clone url git checkout main branch git pull git checkout feature branch git merge main_branch ## if you want to update feature branch with main ## you will get merge conflicts in vscode ## resolve merge conflicts git commit ##check branch name and commit to your feature branch git push feature branch rebase git rebase {branch name} show commit history git log git log -p -1 #to show last commit to delete branch git branch --delete <branchname>","title":"Git"},{"location":"tools/#git-using-python","text":"get current branch name os.system('git rev-parse --abbrev-ref HEAD') get repository name os.system(\"git remote get-url origin\") git commit without verify git commit \u2013no-verify \u2013m \"message\"","title":"git using python"},{"location":"tools/#docker","text":"pull images docker pull <image name> display images docker images display all containers docker ps -a run container docker run -it <repository-name> bash work on container docker exec -it <container_id> bash stop container docker stop <container_id> remove container docker rm <container_id> delete all containers docker container prune -f``` - kill container docker kill - Stop all the containers ```docker stop $(docker ps -a -q)``` - Remove all the containers ```docker rm $(docker ps -a -q)``` - check containers with runnning docker ps docker container ls - to remove image docker container ls docker rmi docker image rm - Remove all unused containers, volumes, networks and images docker system prune -a --volumes - Docker inspect container docker inspect - to push images in docker hub docker container ls docker commit imuser/ansible-server(any iimage name) docker images( to chechk) docker login docker push imuser/ansible-server(we can give any iimage name) ## Python ### Create Env - create virtual environment runnning python -m venv venv - start environment based on your command line tool, e.g. .\\venv\\Scripts\\activate - check installed modules pip list - create requirement.txt file pip freeze > requirements.txt - install all relevant modules via pip pip install -r requirements.txt - install python package for local development and usage: python -m pip install -e . ```","title":"Docker"},{"location":"usage/","text":"Grab the Python stuff here ...\ud83d\ude09 Usage doc is to state syntax for builtin funct. and modules Builtin or Modules use of init .py file init .py is used t import packages from a particular directory Module: A single python script. Package: A collection of modules. (for eg: directory in which all py files contains) click here for the documentation for usage of init file. args and kwargs In Python, we can pass a variable number of arguments to a function using special symbols. There are two special symbols: *args (Non Keyword Arguments) **kwargs (Keyword Arguments) We use *args and kwargs as an argument when we are unsure about the number of arguments to pass in the functions. magic methods num=10 num + 5 15 num.__add__(5) ##magic method __add__ 15 click here for more magic method isinstance The isinstance() function returns True if the specified object is of the specified type, otherwise False syntax isinstance(object, type) decorator decorators allow you to change the behavior of a function without modifying the function itself. syntax def my_decorator_func(func): def wrapper_func(): # Do something before the function. func() # Do something after the function. return wrapper_func #using decoarate @my_decorator_func def my_func(): pass Lambda lambda basic form lambda (parameters):(return_Val) lambda x,y:x+y Value will return automatically lambda is used to avoid a small function so that it can be placed in one line Filter Filtering in python (which is similar to mapping) Syntax filter(some_function,my_list) Mapping Mapping is to map the values with respt. parameters Syntax map(some_function,my_list) for eg: mapping the dict. Key and vlaues Mapping is used by calling it on the List and passing it some function List Comprehension the simply readable way of transforming elements in list either by mapping or filtering List Comprehension Syntax for Mapping var = class_type({return_value} {for loop}) List Comprehension Syntax for Filtering var = class_type({return_value} {for loop} {condition}) read and write syntax f = open({file_path},{file_mode}) dataclass this module define classes that only act as data containers and when we do that, we spend a consequent amount of time writing boilerplate code with tons of arguments, an ugly init method and many overridden functions. When you use dataclasses, you first have to import dataclass and then use it as a decorator before the class you define. syntax from dataclasses import dataclass ##import dataclass @dataclass class Book: title : str ###variables are define without using __init__ instance author : str price : float book=Book(\"title\",\"author\",price) print(book.price) ###print values logging Logging is a means of tracking events that happen when some software runs. The software\u2019s developer adds logging calls to their code to indicate that certain events have occurred.Due to this logging method, the record of events can be saved unlike print which only displays the content in console. different level of logging, also logging methods should be in following order: Basic logging method logging.debug('debug') logging.info(\"info\") logging.warning('warn') logging.error('error') logging.critical('critical') Logging with configuration import logging logging.basicConfig(level=logging.INFO, filename=\"{filename.log}\",filemode=\"w\", format=\"%(asctime)s - %(levelname)s - %(funcName)s - %(lineno)s - %(message)s\") ##funcname display the actual fnction name regex The re module offers a set of functions that allows us to search a string for a match: most used modules in regex are : Function Description findall > Returns a list containing all matches ##returns all matched searched elements search > Returns a Match object if there is a match anywhere in the string split > Returns a list where the string has been split at each match sub > Replaces one or many matches with a string finditer >","title":"Usage"},{"location":"usage/#grab-the-python-stuff-here","text":"Usage doc is to state syntax for builtin funct. and modules","title":"Grab the Python stuff here ...\ud83d\ude09"},{"location":"usage/#builtin-or-modules","text":"","title":"Builtin or Modules"},{"location":"usage/#use-of-initpy-file","text":"init .py is used t import packages from a particular directory Module: A single python script. Package: A collection of modules. (for eg: directory in which all py files contains) click here for the documentation for usage of init file.","title":"use of init.py file"},{"location":"usage/#args-and-kwargs","text":"In Python, we can pass a variable number of arguments to a function using special symbols. There are two special symbols: *args (Non Keyword Arguments) **kwargs (Keyword Arguments) We use *args and kwargs as an argument when we are unsure about the number of arguments to pass in the functions.","title":"args and kwargs"},{"location":"usage/#magic-methods","text":"num=10 num + 5 15 num.__add__(5) ##magic method __add__ 15 click here for more magic method","title":"magic methods"},{"location":"usage/#isinstance","text":"The isinstance() function returns True if the specified object is of the specified type, otherwise False syntax isinstance(object, type)","title":"isinstance"},{"location":"usage/#decorator","text":"decorators allow you to change the behavior of a function without modifying the function itself. syntax def my_decorator_func(func): def wrapper_func(): # Do something before the function. func() # Do something after the function. return wrapper_func #using decoarate @my_decorator_func def my_func(): pass","title":"decorator"},{"location":"usage/#lambda","text":"lambda basic form lambda (parameters):(return_Val) lambda x,y:x+y Value will return automatically lambda is used to avoid a small function so that it can be placed in one line","title":"Lambda"},{"location":"usage/#filter","text":"Filtering in python (which is similar to mapping) Syntax filter(some_function,my_list)","title":"Filter"},{"location":"usage/#mapping","text":"Mapping is to map the values with respt. parameters Syntax map(some_function,my_list) for eg: mapping the dict. Key and vlaues Mapping is used by calling it on the List and passing it some function","title":"Mapping"},{"location":"usage/#list-comprehension","text":"the simply readable way of transforming elements in list either by mapping or filtering List Comprehension Syntax for Mapping var = class_type({return_value} {for loop}) List Comprehension Syntax for Filtering var = class_type({return_value} {for loop} {condition})","title":"List Comprehension"},{"location":"usage/#read-and-write","text":"syntax f = open({file_path},{file_mode})","title":"read and write"},{"location":"usage/#dataclass","text":"this module define classes that only act as data containers and when we do that, we spend a consequent amount of time writing boilerplate code with tons of arguments, an ugly init method and many overridden functions. When you use dataclasses, you first have to import dataclass and then use it as a decorator before the class you define. syntax from dataclasses import dataclass ##import dataclass @dataclass class Book: title : str ###variables are define without using __init__ instance author : str price : float book=Book(\"title\",\"author\",price) print(book.price) ###print values","title":"dataclass"},{"location":"usage/#logging","text":"Logging is a means of tracking events that happen when some software runs. The software\u2019s developer adds logging calls to their code to indicate that certain events have occurred.Due to this logging method, the record of events can be saved unlike print which only displays the content in console. different level of logging, also logging methods should be in following order: Basic logging method logging.debug('debug') logging.info(\"info\") logging.warning('warn') logging.error('error') logging.critical('critical') Logging with configuration import logging logging.basicConfig(level=logging.INFO, filename=\"{filename.log}\",filemode=\"w\", format=\"%(asctime)s - %(levelname)s - %(funcName)s - %(lineno)s - %(message)s\") ##funcname display the actual fnction name","title":"logging"},{"location":"usage/#regex","text":"The re module offers a set of functions that allows us to search a string for a match: most used modules in regex are : Function Description findall > Returns a list containing all matches ##returns all matched searched elements search > Returns a Match object if there is a match anywhere in the string split > Returns a list where the string has been split at each match sub > Replaces one or many matches with a string finditer >","title":"regex"}]}